<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>08. 评测基准 - World Model Guide</title>
    <link rel="stylesheet" href="css/style.css">
    <script>
        window.MathJax = {
            tex: { inlineMath: [['\\(', '\\)'], ['$', '$']] },
            options: { skipHtmlTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'] }
        };
    </script>
    <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <style>
        .paper-card {
            background: white;
            border: 1px solid var(--border);
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin: 1.5rem 0;
            box-shadow: var(--card-shadow);
        }

        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .paper-title {
            font-size: 1.2rem;
            font-weight: 700;
            color: var(--text-primary);
            margin-bottom: 0.5rem;
        }

        .paper-title a {
            color: var(--accent);
            text-decoration: none;
        }

        .paper-title a:hover {
            text-decoration: underline;
        }

        .paper-meta {
            font-size: 0.85rem;
            color: var(--text-secondary);
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
            margin-bottom: 1rem;
        }

        .paper-tag {
            background: #e0f2fe;
            color: #0369a1;
            padding: 0.2rem 0.6rem;
            border-radius: 1rem;
            font-size: 0.75rem;
            font-weight: 600;
        }

        .paper-tag.github {
            background: #f0fdf4;
            color: #15803d;
        }

        .paper-tag.open {
            background: #f0fdf4;
            color: #15803d;
        }

        .paper-tag.closed {
            background: #fee2e2;
            color: #b91c1c;
        }

        .paper-tag.unknown {
            background: #f1f5f9;
            color: #475569;
        }

        .paper-tag.new {
            background: #fef3c7;
            color: #b45309;
        }

        .paper-tag a {
            color: inherit;
            text-decoration: none;
        }

        .paper-tag a:hover {
            text-decoration: underline;
        }

        .dual-view {
            display: flex;
            flex-direction: column;
            gap: 1rem;
            margin: 1rem 0;
        }

        .layman-view {
            background: #f0fae6;
            padding: 1rem;
            border-radius: 0.5rem;
            border-left: 4px solid #22c55e;
        }

        .pro-view {
            background: #fff1f2;
            padding: 1rem;
            border-radius: 0.5rem;
            border-left: 4px solid #e11d48;
        }

        .commentary {
            background: #fce7f3;
            padding: 1rem;
            border-radius: 0.5rem;
            border-left: 4px solid #db2777;
            margin-top: 1rem;
        }

        .commentary-title {
            font-weight: 700;
            font-size: 0.9rem;
            margin-bottom: 0.5rem;
            color: #9d174d;
            letter-spacing: 0.2px;
        }

        .source-note {
            margin-top: 0.75rem;
            font-size: 0.85rem;
            color: var(--text-secondary);
        }

        .source-note a {
            color: var(--accent);
        }

        .month-index {
            display: flex;
            flex-wrap: wrap;
            gap: 0.5rem;
            margin: 0.75rem 0 1.25rem;
            padding: 0.75rem;
            background: #f8fafc;
            border: 1px solid var(--border);
            border-radius: 0.75rem;
        }

        .month-link {
            display: inline-flex;
            align-items: center;
            padding: 0.35rem 0.75rem;
            border-radius: 999px;
            border: 1px solid var(--border);
            background: white;
            color: var(--text-secondary);
            text-decoration: none;
            font-weight: 600;
            font-size: 0.85rem;
        }

        .month-link:hover {
            border-color: var(--accent);
            color: var(--accent);
        }

        .last-updated {
            background: #fef3c7;
            border: 1px solid #fcd34d;
            padding: 1rem;
            border-radius: 0.5rem;
            font-size: 0.9rem;
            margin-bottom: 2rem;
        }

        .view-title {
            font-weight: 700;
            font-size: 0.9rem;
            margin-bottom: 0.5rem;
        }

        .layman-view .view-title {
            color: #166534;
        }

        .pro-view .view-title {
            color: #9f1239;
        }
    </style>
</head>

<body>
    <div class="aurora-bg">
        <div class="aurora-blob aurora-blob-1"></div>
        <div class="aurora-blob aurora-blob-2"></div>
        <div class="aurora-blob aurora-blob-3"></div>
        <div class="aurora-blob aurora-blob-4"></div>
    </div>
    <nav class="sidebar">
        <a href="index.html" class="brand">🚀 World Model Guide</a>
        <ul class="nav-links">
            <li class="nav-item"><a href="index.html" class="nav-link">00. 概览 (Overview)</a></li>
            <li class="nav-item"><a href="01_industry.html" class="nav-link">01. 行业全景 (Landscape)</a></li>
            <li class="nav-item"><a href="02_product.html" class="nav-link">02. 产品深度 (Deep Dive)</a></li>
            <li class="nav-item"><a href="03_architecture.html" class="nav-link">03. 技术架构 (Architecture)</a></li>
            <li class="nav-item"><a href="04_data.html" class="nav-link">04. 数据工程 (Data Bible)</a></li>
            <li class="nav-item"><a href="05_roadmap.html" class="nav-link">05. 落地路线 (Roadmap)</a></li>
            <li class="nav-item"><a href="06_companies.html" class="nav-link">06. 公司调研 (Companies)</a></li>
            <li class="nav-item"><a href="07_paper_tracker.html" class="nav-link">07. 论文追踪 (Papers)</a></li>
            <li class="nav-item"><a href="10_benchmarks.html" class="nav-link active">08. 评测基准 (Benchmarks)</a></li>
            <li class="nav-item"><a href="08_community.html" class="nav-link">09. 社区动态 (Community)</a></li>
            <li class="nav-item"><a href="09_update_log.html" class="nav-link">10. 更新日志 (Updates)</a></li>
            <li class="nav-item"><a href="references.html" class="nav-link">附录：参考文献 (Refs)</a></li>
        </ul>
    </nav>

    <main class="main-content">
        <h1>08. 评测基准与评测方法 (Benchmarks & Evaluation)</h1>
        <p>本章用于补齐“论文里用到的评测到底在测什么、怎么测、测出来代表什么”。每个基准/评测方法都按论文追踪同款结构：🟢通俗→🔴专业→🧠点评，并附可追溯链接。</p>

        <div class="last-updated">
            ⏰ <strong>最后更新时间</strong>: 2026-01-26 | 本页内容将每日更新「业界 Benchmarks/评测方法：使用方式、评测重点与引用链接」。
        </div>

        <section>
            <h2>🗂️ 导航</h2>
            <div class="month-index" role="navigation" aria-label="评测导航">
                <a class="month-link" href="#bm-vbench">VBench / VBench++ / VBench-Long</a>
                <a class="month-link" href="#bm-vbench2">VBench-2.0（Intrinsic Faithfulness）</a>
                <a class="month-link" href="#bm-physbench">PhysBench（物理理解）</a>
                <a class="month-link" href="#bm-fvd">FVD（Fréchet Video Distance）</a>
                <a class="month-link" href="#bm-vlm-judge">VLM-as-a-Judge / Reward</a>
            </div>

            <!-- 内容从 07_paper_tracker.html 迁移而来 -->
            <div class="paper-card" id="bm-vbench">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://github.com/Vchitect/VBench" target="_blank">VBench / VBench++ / VBench-Long：视频生成评测“多维体检表”</a></div>
                        <div class="paper-meta">
                            <span>📅 2023-11-29</span>
                            <span>📄 <a href="https://arxiv.org/abs/2311.17982" target="_blank">arXiv:2311.17982</a></span>
                            <span class="paper-tag">判定者：模型</span>
                            <span class="paper-tag"><a href="https://vchitect.github.io/VBench-project/" target="_blank">Project</a></span>
                            <span class="paper-tag github"><a href="https://github.com/Vchitect/VBench" target="_blank">GitHub</a></span>
                            <span class="paper-tag open"><a href="https://huggingface.co/spaces/Vchitect/VBench_Leaderboard" target="_blank">Leaderboard</a></span>
                            <span class="paper-tag open"><a href="https://github.com/Vchitect/VBench/tree/master/vbench2_beta_long" target="_blank">VBench-Long</a></span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>VBench 像一个“视频体检表”：不是只问“好不好看”，而是把视频质量拆成很多项（比如主体是否一直是同一个人、动作是否自然、画面有没有闪烁、是否符合文本描述），然后给每一项打分。这样你就能知道模型到底“强在哪、弱在哪”。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>评测对象</strong>：主要面向 T2V/I2V 的生成结果（prompt + 生成视频），输出为多维度分数与汇总分。<sup><a href="https://github.com/Vchitect/VBench" target="_blank">[来源]</a></sup></li>
                            <li><strong>评测重点</strong>：把“视频生成质量”拆成多维指标，通常可分为两类：Video Quality（清晰度/美学/一致性/闪烁等）与 Video-Condition Consistency（与 prompt 的语义一致、动作一致等）。<sup><a href="https://arxiv.org/abs/2311.17982" target="_blank">[来源]</a></sup></li>
                            <li><strong>长视频扩展</strong>：VBench++ 提供 VBench-Long，用于评测长视频生成模型（对分钟级/更长输出更敏感）。<sup><a href="https://github.com/Vchitect/VBench/tree/master/vbench2_beta_long" target="_blank">[来源]</a></sup></li>
                            <li><strong>使用方式（怎么跑）</strong>：
                                <ul style="margin: 0.5rem 0 0.25rem; padding-left: 1.25rem;">
                                    <li>准备：生成视频 + 对应 prompt（以及可选的条件信息）。</li>
                                    <li>按 repo README 运行 evaluator 脚本计算各维度分数；提交 leaderboard 前通常会计算 Total/Quality/Semantic 等汇总指标。<sup><a href="https://github.com/Vchitect/VBench" target="_blank">[来源]</a></sup></li>
                                    <li>如果想快速对比模型，可直接参考官方 HuggingFace leaderboard/Arena 的公开结果与样例视频。<sup><a href="https://huggingface.co/spaces/Vchitect/VBench_Leaderboard" target="_blank">[来源]</a></sup></li>
                                </ul>
                            </li>
                        </ul>
                        <div class="source-note">
                            <strong>来源定位</strong>：<a href="https://arxiv.org/abs/2311.17982" target="_blank">论文</a>（总体设计与维度拆分）；<a href="https://github.com/Vchitect/VBench" target="_blank">GitHub README</a>（VBench / VBench++ / VBench-Long 组件位置、leaderboard 与运行方式、Arena 链接）。
                        </div>

                        <figure style="margin: 0.75rem 0 0;">
                            <figcaption style="font-size: 0.85rem; color: var(--text-secondary); margin-bottom: 0.5rem;">
                                <strong>图：VBench 的“多维评测”流水线（简化）</strong>
                            </figcaption>
                            <svg viewBox="0 0 900 220" width="100%" height="auto" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="VBench evaluation pipeline">
                                <defs>
                                    <marker id="arrowBM1" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                        <path d="M0,0 L10,3 L0,6 Z" fill="#64748b" />
                                    </marker>
                                </defs>
                                <rect x="20" y="40" width="240" height="70" rx="12" fill="#f0fae6" stroke="#22c55e" />
                                <text x="35" y="68" font-size="13" fill="#166534" font-weight="700">Inputs</text>
                                <text x="35" y="90" font-size="12" fill="#166534">prompt + generated video</text>

                                <rect x="290" y="40" width="310" height="70" rx="12" fill="#e0f2fe" stroke="#0369a1" />
                                <text x="305" y="68" font-size="13" fill="#0369a1" font-weight="700">Multi-dimension evaluators</text>
                                <text x="305" y="90" font-size="12" fill="#0369a1">quality + condition-consistency</text>

                                <rect x="630" y="40" width="250" height="70" rx="12" fill="#fff1f2" stroke="#e11d48" />
                                <text x="645" y="68" font-size="13" fill="#9f1239" font-weight="700">Outputs</text>
                                <text x="645" y="90" font-size="12" fill="#9f1239">dimension scores + total score</text>

                                <line x1="260" y1="75" x2="290" y2="75" stroke="#64748b" stroke-width="2" marker-end="url(#arrowBM1)" />
                                <line x1="600" y1="75" x2="630" y2="75" stroke="#64748b" stroke-width="2" marker-end="url(#arrowBM1)" />

                                <rect x="20" y="135" width="860" height="55" rx="12" fill="#f8fafc" stroke="#e2e8f0" />
                                <text x="35" y="168" font-size="12" fill="#334155">关键解读：看“总分”只能排序；看“分维度曲线”才能定位模型瓶颈（主体漂移/运动不稳/语义不对/闪烁等）。</text>
                            </svg>
                        </figure>
                    </div>
                </div>

                <div class="commentary">
                    <div class="commentary-title">🧠 AntiGravity's Commentary</div>
                    <p style="margin:0;">VBench 是行业里最接近“可诊断”的自动评测体系之一：它的价值不在 rank，而在让你能把工程优化对齐到具体维度（例如先解决主体一致性，再谈美学）。但要小心“刷分陷阱”：任何自动指标都会诱导过拟合，建议用它做回归测试（regression）与方向性诊断，而不是当唯一 KPI。</p>
                </div>
            </div>

            <div class="paper-card" id="bm-vbench2">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://vchitect.github.io/VBench-2.0-project/" target="_blank">VBench-2.0：评测“内在真实度”（物理/常识/人类运动/创造性）</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-03-27</span>
                            <span>📄 <a href="https://arxiv.org/abs/2503.21755" target="_blank">arXiv:2503.21755</a></span>
                            <span class="paper-tag">判定者：模型</span>
                            <span class="paper-tag"><a href="https://vchitect.github.io/VBench-2.0-project/" target="_blank">Project</a></span>
                            <span class="paper-tag github"><a href="https://github.com/Vchitect/VBench/tree/master/VBench-2.0" target="_blank">GitHub</a></span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>如果说 VBench 更像“画质体检”，VBench-2.0 更像“世界观体检”：它更关心视频是否<strong>符合物理和常识</strong>、人的动作是不是合理、剧情是否自洽，而不仅是清晰好看。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>评测重点（Intrinsic Faithfulness）</strong>：从“像不像”推进到“对不对”，覆盖 commonsense、physics、human motion、composition 等更深层能力维度。<sup><a href="https://arxiv.org/abs/2503.21755" target="_blank">[来源]</a></sup></li>
                            <li><strong>使用方式（怎么用）</strong>：在同一仓库内以独立模块维护（`VBench-2.0/`），按维度或全量评测运行；官方还支持评测自定义视频（single dimension / customized videos）。<sup><a href="https://github.com/Vchitect/VBench/tree/master/VBench-2.0" target="_blank">[来源]</a></sup></li>
                            <li><strong>落地建议</strong>：当你做“世界模型/交互视频”时，优先把 VBench-2.0 当作“物理/常识/人类动作一致性”的回归测试，而不是泛画质评分。因为这类能力更接近长期稳定与可交互的真实瓶颈。</li>
                        </ul>
                        <div class="source-note">
                            <strong>来源定位</strong>：<a href="https://arxiv.org/abs/2503.21755" target="_blank">论文</a>（intrinsic faithfulness 的维度设计与动机）；<a href="https://github.com/Vchitect/VBench/tree/master/VBench-2.0" target="_blank">GitHub</a>（模块位置、评测自定义视频的说明与脚本入口）。
                        </div>
                    </div>
                </div>

                <div class="commentary">
                    <div class="commentary-title">🧠 AntiGravity's Commentary</div>
                    <p style="margin:0;">“内在真实度”是世界模型最终会被追问的能力：哪怕短视频看起来很美，只要物理/常识经常崩，交互就不可用。VBench-2.0 的意义是把这类能力变成可测量的维度；但仍要注意：复杂叙事/长链因果往往是现有基准最薄弱的一环，别被单一汇总分掩盖了失败模式。</p>
                </div>
            </div>

            <div class="paper-card" id="bm-physbench">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://physbench.github.io/" target="_blank">PhysBench：物理世界理解评测（视频-图像-文本混合）</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-01-27</span>
                            <span>📄 <a href="https://arxiv.org/abs/2501.16411" target="_blank">arXiv:2501.16411</a></span>
                            <span class="paper-tag">判定者：传统算法</span>
                            <span class="paper-tag"><a href="https://physbench.github.io/" target="_blank">Project</a></span>
                            <span class="paper-tag github"><a href="https://github.com/physical-superintelligence-lab/PhysBench" target="_blank">GitHub</a></span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>PhysBench 用来测试模型懂不懂“物理常识”：比如东西会不会掉、碰撞会怎样、物体关系和运动是否合理。它更像给模型做“物理理解考试”。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>评测重点</strong>：面向 VLM 的物理世界理解能力评测，覆盖物体属性/关系/场景/动力学等维度，并提供系统化评测入口。<sup><a href="https://arxiv.org/abs/2501.16411" target="_blank">[来源]</a></sup></li>
                            <li><strong>使用方式</strong>：以官方仓库为准，按其数据准备与评测脚本跑基准；实践中常把它当作“物理推理能力”的回归测试集，用来验证模型更新是否真的提升物理理解而非只提升语言模板。<sup><a href="https://github.com/physical-superintelligence-lab/PhysBench" target="_blank">[来源]</a></sup></li>
                            <li><strong>与世界模型的关系</strong>：当论文宣称“更懂物理/更真实”，建议把 PhysBench/PhysGame 这类数据与评测，作为对齐物理真实度的外部证据，而不是只看 VBench 总分。</li>
                        </ul>
                        <div class="source-note">
                            <strong>来源定位</strong>：<a href="https://arxiv.org/abs/2501.16411" target="_blank">论文</a>（任务定义/维度/结论）；<a href="https://github.com/physical-superintelligence-lab/PhysBench" target="_blank">GitHub</a>（数据与评测脚本入口）。
                        </div>
                    </div>
                </div>

                <div class="commentary">
                    <div class="commentary-title">🧠 AntiGravity's Commentary</div>
                    <p style="margin:0;">物理理解类基准最大的价值是把“看起来合理”拆成可检验的问题集。对世界模型团队来说，建议把它用作：① 训练/对齐数据的目标分布参考；② 推理链路改动后的回归测试。注意避免只追求答题技巧：真正要的是跨场景的物理一致性，而不是模板化 QA。</p>
                </div>
            </div>

            <div class="paper-card" id="bm-fvd">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/1812.01717" target="_blank">FVD：Fréchet Video Distance（经典自动指标）</a></div>
                        <div class="paper-meta">
                            <span>📅 2018-12-03</span>
                            <span>📄 <a href="https://arxiv.org/abs/1812.01717" target="_blank">arXiv:1812.01717</a></span>
                            <span class="paper-tag">判定者：传统算法</span>
                            <span class="paper-tag"><a href="https://github.com/tensorflow/gan" target="_blank">TF-GAN</a></span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>FVD 是“自动打分”的老指标：它用一个视频特征提取器把真实视频和生成视频都变成向量分布，然后看两者差得有多远。分数越低通常越接近真实。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>核心定义</strong>：在视频特征空间里计算 Fréchet 距离，本质与 FID 类似：\(\|\mu_r-\mu_g\|_2^2 + \mathrm{Tr}(\Sigma_r+\Sigma_g-2(\Sigma_r\Sigma_g)^{1/2})\)。<sup><a href="https://arxiv.org/abs/1812.01717" target="_blank">[来源]</a></sup></li>
                            <li><strong>使用方式</strong>：关键是统一特征提取器与采样设置（帧率/时长/裁剪），否则分数不可比；常见实现可参考 TF-GAN 中的 FVD/FID 相关工具。<sup><a href="https://github.com/tensorflow/gan" target="_blank">[来源]</a></sup></li>
                            <li><strong>评测重点与局限</strong>：FVD 更像“总体分布相似度”，对特定失败模式（身份漂移/动作因果错误/物理不合理）不敏感；因此更适合作为 baseline 指标，与 VBench 等诊断型评测互补。</li>
                        </ul>
                        <div class="source-note">
                            <strong>来源定位</strong>：<a href="https://arxiv.org/abs/1812.01717" target="_blank">论文</a>（新指标与挑战设置）；<a href="https://github.com/tensorflow/gan" target="_blank">TF-GAN</a>（参考实现与工程化落地）。
                        </div>
                    </div>
                </div>

                <div class="commentary">
                    <div class="commentary-title">🧠 AntiGravity's Commentary</div>
                    <p style="margin:0;">FVD 在很多论文里仍会被引用，但对“可交互世界模型”来说它远远不够：你关心的是长期因果一致性与可控性，而不是平均意义上的分布接近。建议把它留给横向 baseline，对核心结论优先用诊断型（VBench/VBench-2.0）或任务型（PhysBench/PhysGame）评测来支撑。</p>
                </div>
            </div>

            <div class="paper-card" id="bm-vlm-judge">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://openreview.net/forum?id=worldgym" target="_blank">VLM-as-a-Judge / Reward：用“能看视频的模型”当评测器</a></div>
                        <div class="paper-meta">
                            <span>📅 方法条目</span>
                            <span>📄 <a href="https://openreview.net/forum?id=worldgym" target="_blank">OpenReview</a></span>
                            <span class="paper-tag">判定者：模型</span>
                            <span class="paper-tag">Method</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>这是“用 AI 评 AI”：让一个更强的视觉语言模型去看生成的视频，然后打分或给奖励，告诉你“这段视频/这一步动作看起来是不是更合理”。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>使用方式</strong>：把 VLM 当作 reward function / judge：输入（prompt + video 或 state rollout），输出为偏好分/成功率估计/对齐分数；常用于 policy 评估或 best-of-N 选择。<sup><a href="https://openreview.net/forum?id=worldgym" target="_blank">[来源]</a></sup></li>
                            <li><strong>评测重点</strong>：适合评估“任务成功/语义目标是否达成/行为是否合理”等无法用像素指标表达的目标；对交互世界模型尤其关键，因为最终目标往往是“可用性”，不是画质。<sup><a href="https://openreview.net/forum?id=worldgym" target="_blank">[来源]</a></sup></li>
                            <li><strong>关键风险</strong>：judge 本身有偏差（幻觉、偏好漂移、对视觉细节不敏感/过敏感），需要做校准（人类抽检一致性、OOD 测试、reward hacking 风险评估）。</li>
                        </ul>
                        <div class="source-note">
                            <strong>来源定位</strong>：以 WorldGym 为例：<a href="https://openreview.net/forum?id=worldgym" target="_blank">OpenReview</a>（Method/Evaluation：VLM reward 与相关性验证）。通用方法适用于更多“VLM-based evaluation”论文。
                        </div>
                    </div>
                </div>

                <div class="commentary">
                    <div class="commentary-title">🧠 AntiGravity's Commentary</div>
                    <p style="margin:0;">VLM-as-a-judge 是目前最“工程可用”的评测补丁，但必须把它当作<strong>带偏差的代理指标</strong>而不是事实。最佳实践是：VLM 负责高频回归与快速筛选，人类负责小样本高质量审计（并把审计结果反过来校准 judge）。否则最常见的结局就是：模型学会讨好裁判，而不是学会真实世界规律。</p>
                </div>
            </div>
        </section>

        <nav class="chapter-nav">
            <a href="07_paper_tracker.html" class="chapter-nav-link prev">
                <span class="chapter-nav-label">← 上一章</span>
                <span class="chapter-nav-title">07. 论文追踪 (Papers)</span>
            </a>
            <a href="08_community.html" class="chapter-nav-link next">
                <span class="chapter-nav-label">下一章 →</span>
                <span class="chapter-nav-title">09. 社区动态 (Community)</span>
            </a>
        </nav>
    </main>
</body>

</html>
