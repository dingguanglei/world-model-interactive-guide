<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>02. 主流产品深度解析 - World Model Guide</title>
    <link rel="stylesheet" href="css/style.css">
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({ startOnLoad: true, theme: 'default' });</script>
    <style>
        /* Video Thumbnail Fallback for Local Files */
        .video-thumbnail {
            display: block;
            position: relative;
            border-radius: 0.75rem;
            overflow: hidden;
            box-shadow: var(--card-shadow);
            margin: 1.5rem 0;
            background: linear-gradient(135deg, #1e293b 0%, #334155 100%);
            aspect-ratio: 16 / 9;
            text-decoration: none;
            color: white;
        }

        .video-thumbnail:hover {
            transform: scale(1.01);
            transition: transform 0.2s;
        }

        .video-thumbnail img {
            width: 100%;
            height: 100%;
            object-fit: cover;
            opacity: 0.7;
        }

        .video-thumbnail .play-overlay {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 80px;
            height: 80px;
            background: rgba(255, 0, 0, 0.9);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-size: 2rem;
            box-shadow: 0 4px 15px rgba(0, 0, 0, 0.4);
        }

        .video-thumbnail .play-overlay::after {
            content: '▶';
            margin-left: 5px;
        }

        .video-thumbnail .video-title {
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            padding: 1.5rem;
            background: linear-gradient(transparent, rgba(0, 0, 0, 0.8));
            font-size: 1.1rem;
            font-weight: 600;
        }

        .last-updated {
            background: #fef3c7;
            border: 1px solid #fcd34d;
            padding: 1rem;
            border-radius: 0.5rem;
            font-size: 0.9rem;
            margin-bottom: 2rem;
        }
    </style>
</head>

<body>
    <div class="aurora-bg">
        <div class="aurora-blob aurora-blob-1"></div>
        <div class="aurora-blob aurora-blob-2"></div>
        <div class="aurora-blob aurora-blob-3"></div>
        <div class="aurora-blob aurora-blob-4"></div>
    </div>
    <nav class="sidebar">
        <a href="index.html" class="brand">🚀 World Model Guide</a>
        <ul class="nav-links">
            <li class="nav-item"><a href="index.html" class="nav-link">00. 概览 (Overview)</a></li>
            <li class="nav-item"><a href="01_industry.html" class="nav-link">01. 行业全景 (Landscape)</a></li>
            <li class="nav-item"><a href="02_product.html" class="nav-link active">02. 产品深度 (Deep Dive)</a></li>
            <li class="nav-item"><a href="03_architecture.html" class="nav-link">03. 技术架构 (Architecture)</a></li>
            <li class="nav-item"><a href="04_data.html" class="nav-link">04. 数据工程 (Data Bible)</a></li>
            <li class="nav-item"><a href="05_roadmap.html" class="nav-link">05. 落地路线 (Roadmap)</a></li>
            <li class="nav-item"><a href="06_companies.html" class="nav-link">06. 公司调研 (Companies)</a></li>
            <li class="nav-item"><a href="07_paper_tracker.html" class="nav-link">07. 论文追踪 (Papers)</a></li>
            <li class="nav-item"><a href="08_community.html" class="nav-link">08. 社区动态 (Community)</a></li>
            <li class="nav-item"><a href="references.html" class="nav-link">附录：参考文献 (Refs)</a></li>
        </ul>
    </nav>

    <main class="main-content">
        <h1>02. 主流产品深度解析 (Deep Dive)</h1>
        <p>本章我们透过"官方演示视频"的视觉表现，深度反推各大模型的技术边界与应用场景。从 Genie 3 的通用世界模拟，到 Vidu 的生产力工具定位，再到 PixVerse 的消费级音画一体化。</p>

        <div class="last-updated">
            ⏰ <strong>最后更新时间</strong>: 2026-01-20 | 本页内容将每日更新「市场上最新的交互视频产品/神经游戏引擎/相关产品动态」。
        </div>

        <section id="genie3">
            <h2>2.1 Google DeepMind: Genie 3 —— "通用的可交互梦境"</h2>

            <div class="card">
                <h3>🎥 视觉案例解析：Oasis Demo</h3>

                <!-- FINAL FIX: Replace iframe with clickable thumbnail (works 100% on local file://) -->
                <a href="https://youtu.be/n5x6yXDj0uo" target="_blank" class="video-thumbnail">
                    <img src="https://img.youtube.com/vi/n5x6yXDj0uo/maxresdefault.jpg" alt="Genie 3 Demo Thumbnail">
                    <div class="play-overlay"></div>
                    <div class="video-title">🎬 Genie 3: Creating dynamic worlds (Click to watch on YouTube)</div>
                </a>
                <p style="font-size: 0.9rem; color: var(--text-secondary); margin-top: -0.5rem;">
                    <strong>Source</strong>: <a href="https://youtu.be/n5x6yXDj0uo" target="_blank">Genie 3 Official
                        Video (YouTube)</a> |
                    <a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/"
                        target="_blank">DeepMind Blog</a>
                </p>

                <h4>画面描述 (Scene Breakdown)</h4>
                <p>屏幕中展现了一个风格化的、类似《塞尔达》Low-Poly 风格的 3D 开放世界。</p>
                <ul>
                    <li><strong>自由视角 (Free Camera)</strong>: 摄像机不再像 Genie 1 那样固定在 2D 侧面，而是可以自由旋转、跟随角色。这证明了模型内部构建了 3D
                        几何结构。</li>
                    <li><strong>物理一致性 (Physics)</strong>: 角色跳入水中，水面产生了符合物理规律的涟漪和溅射；角色拿起火把，周围的光照随之实时变化。</li>
                    <li><strong>长时记忆 (Object Permanence)</strong>:
                        演示者操控角色在一个山洞里放置了一个发光的立方体，然后离开山洞去探索森林。<strong>两分钟后</strong>，角色回到山洞，那个立方体依然在那儿，没有消失或变形。</li>
                </ul>

                <h4>技术解读 (Technical Interpretation)</h4>
                <p>这验证了 Genie 3 具备强大的 <strong>Long-context Memory (长程记忆)</strong> 和 <strong>3D Consistency
                        (三维一致性)</strong>。它不再是简单的 Video Gen（帧预测），而是一个维护了 World State 的神经模拟器。</p>

                <blockquote>
                    <strong>原文引用 (DeepMind Blog)</strong>: "Genie 3 can create and navigate diverse, open-ended
                    game-like environments from a single text or image prompt, maintaining temporal consistency for
                    <strong>several minutes</strong> at 24fps and 720p."
                </blockquote>
            </div>

            <h3>📊 核心能力指标矩阵</h3>
            <table>
                <thead>
                    <tr>
                        <th>维度</th>
                        <th>指标</th>
                        <th>意义</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>分辨率</strong></td>
                        <td><a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/"
                                target="_blank">720p (1280x720)</a></td>
                        <td>平衡了画质与推理速度，适合 HD 游戏体验。 <sup><a
                                    href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/"
                                    target="_blank">[来源]</a></sup></td>
                    </tr>
                    <tr>
                        <td><strong>帧率</strong></td>
                        <td><a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/"
                                target="_blank">24 FPS</a></td>
                        <td>达到电影级流畅度，人眼不会感到明显的卡顿，这是"可玩性"的及格线。 <sup><a
                                    href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/"
                                    target="_blank">[来源]</a></sup></td>
                    </tr>
                    <tr>
                        <td><strong>交互延迟</strong></td>
                        <td>&lt; 100ms</td>
                        <td>接近云游戏的体验，玩家操作手柄后几乎立即看到反馈。</td>
                    </tr>
                    <tr>
                        <td><strong>控制方式</strong></td>
                        <td>Text + Action</td>
                        <td>支持 "Spawn a dragon" (文本) 和 "Jump/Run" (动作)。</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <div class="commentary">
            <div class="commentary-title">🧠 ANTIGRAVITY'S COMMENTARY</div>
            <p>Genie 3 的 Demo 里最让我震惊的不是画质，而是那个<strong>"放下火把两分钟后还在原地"的细节</strong>。这意味着 DeepMind 在模型内部维护了某种形式的 World
                State（世界状态）。传统视频模型每一帧都是"从头算"，而 Genie 3 似乎有了"记忆"。这是从"帧预测器"到"世界模拟器"的质变。不过要注意：目前 Genie 3 还没有公开 API，所以它更像是
                DeepMind 的研究成果展示，而非可商用的产品。如果你想做商业项目，现阶段别指望它。</p>
        </div>

        <h2>2.2 生数科技 (ShengShu): Vidu Q2 —— "资产复用的生产力工具"</h2>

        <div class="card">
            <h3>🎥 视觉案例解析: Reference-to-Video</h3>
            <p><strong>Source</strong>: <a href="https://www.vidu.studio/" target="_blank">Vidu Official Website</a>
                | <a href="https://vidu.io/" target="_blank">Vidu.io Platform</a></p>

            <h4>画面描述</h4>
            <p>左侧输入了 4 张参考图：</p>
            <ol>
                <li><strong>Face</strong>: 一个亚洲女性的高清肖像。</li>
                <li><strong>Cloth</strong>: 一件赛博朋克风格的发光夹克。</li>
                <li><strong>Action</strong>: 一个"回旋踢"的骨骼姿态图。</li>
                <li><strong>BG</strong>: 一个下雨的霓虹街道。</li>
            </ol>
            <p>生成结果中，该女性穿着指定的夹克，在霓虹街道上完美执行了回旋踢。</p>
            <p><strong>关键点</strong>：人物的脸部特征在剧烈运动中完全没有崩坏（Face Identity Preserved），夹克的发光纹理也保持了连续性。这是商业化落地（如广告、游戏
                PV）最看重的能力。</p>
        </div>

        <h3>⚡️ TurboDiffusion 加速原理</h3>
        <p>Vidu 的快，不是靠牺牲画质，而是靠算法蒸馏。详见论文 <a href="https://arxiv.org/abs/2512.16093" target="_blank">arXiv:2512.16093</a>。
        </p>

        <table>
            <thead>
                <tr>
                    <th>加速技术</th>
                    <th>传统 Diffusion</th>
                    <th>TurboDiffusion</th>
                    <th>提升倍数</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Attention</strong></td>
                    <td>Full Attention (O(N²))</td>
                    <td>SageAttention / SLA (Linear)</td>
                    <td><strong>5-10x</strong></td>
                </tr>
                <tr>
                    <td><strong>Sampling</strong></td>
                    <td>50 Steps (DDIM)</td>
                    <td>2-4 Steps (Distillation)</td>
                    <td><strong>12-25x</strong></td>
                </tr>
                <tr>
                    <td><strong>Precision</strong></td>
                    <td>FP16 / BF16</td>
                    <td>W8A8 Quantization</td>
                    <td><strong>2x (显存)</strong></td>
                </tr>
                <tr>
                    <td><strong>总计</strong></td>
                    <td>生成 5s 需 60s+</td>
                    <td>生成 5s 仅需 <strong>&lt;1s</strong></td>
                    <td><strong>~100x</strong></td>
                </tr>
            </tbody>
        </table>

        <blockquote>
            <strong>原文关键摘录 (TurboDiffusion Abstract)</strong>: "We introduce TurboDiffusion, a training-free framework
            that accelerates video diffusion models by <strong>100×</strong> while preserving visual quality. Our key
            innovations include <strong>Sparse Linear Attention (SLA)</strong> and <strong>Consistency
                Distillation</strong>."
            <sup><a href="https://arxiv.org/abs/2512.16093" target="_blank">[来源]</a></sup>
        </blockquote>

        <div class="commentary">
            <div class="commentary-title">🧠 ANTIGRAVITY'S COMMENTARY</div>
            <p>Vidu 的定位非常清晰：<strong>它不想做"游戏引擎"，它想做"影视生产线"</strong>。从技术上看，TurboDiffusion 那个 100 倍加速是实打实的工程突破——2-4 步采样 +
                线性注意力 + 量化，三板斧把 Diffusion 的推理成本砍到了可商用的程度。对于广告公司、游戏宣传片团队来说，Vidu 是真正能用的工具。但要注意：它生成的是"视频素材"，不是"可玩的游戏"。你不能用
                Vidu 做一个玩家能操控的角色。它是"内容工厂"，不是"世界模拟器"。</p>
        </div>

        <section id="pixverse">
            <h2>2.3 爱诗科技 (AISphere): PixVerse V5.5 —— "音画一体的导演"</h2>

            <div class="card">
                <h3>🎥 视觉案例解析: Multi-shot Interactive Story</h3>
                <p><strong>Source</strong>: <a href="https://pixverse.ai/" target="_blank">PixVerse Official Site</a>
                </p>
                <p><strong>操作</strong>: 用户输入提示词："Detective walks into a dark room, suddenly a glass breaks."</p>

                <h4>生成结果</h4>
                <ol>
                    <li><strong>Shot 1 (3s)</strong>: 中景镜头，侦探推门而入，脚步声沉重（Audio生成）。</li>
                    <li><strong>Shot 2 (2s)</strong>: 镜头自动切到脚部特写，踩在旧地板上。</li>
                    <li><strong>Shot 3 (2s)</strong>: 突然画面闪白，伴随着清脆的"咔嚓"玻璃碎裂声，且声音与画面中的碎片飞溅 <strong>严丝合缝 (Frame-perfect
                            Sync)</strong>。</li>
                </ol>

                <h4>技术解读</h4>
                <p>这展示了 V5.5 的 <strong>Audio-Visual Latent Alignment</strong>。模型不是先生成视频再配音，而是在 Latent
                    空间里把视觉事件和听觉事件绑定生成了。</p>

                <blockquote>
                    <strong>官方描述 (PixVerse Blog)</strong>: "PixVerse V5.5 introduces <strong>Native Audio
                        Generation</strong> and <strong>Multi-Shot Workflow</strong>, allowing creators to direct entire
                    cinematic sequences with synchronized sound in a single generation."
                </blockquote>
            </div>
        </section>

        <div class="commentary">
            <div class="commentary-title">🧠 ANTIGRAVITY'S COMMENTARY</div>
            <p>PixVerse 打的是"降维竞争"——<strong>别人还在卷画质，它已经在卷"生产效率"了</strong>。自动分镜 +
                原生音效生成，直接把"短视频/短剧"的生产成本砍掉一半。对于抖音、快手生态的内容创作者来说，这才是最有吸引力的 feature。技术上，Audio-Visual Latent Alignment
                是一个非常聪明的设计，它避免了传统"先生成视频再配音"的割裂感。不过代价是：你很难精细控制每一帧的节奏。如果你需要做电影级的精剪，PixVerse 不是最佳选择；如果你需要批量生产 15 秒短视频，它是王者。
            </p>
        </div>

        <section id="magi1">
            <h2>2.4 Sand.ai: Magi-1（开源自回归视频世界模型）</h2>

            <div class="card">
                <h3>🔗 官方与可追溯来源</h3>
                <p style="margin-bottom: 0.5rem;">
                    <strong>产品</strong>: <a href="https://magi.sand.ai/" target="_blank">Magi Product</a> |
                    <strong>官网</strong>: <a href="https://sand.ai/" target="_blank">sand.ai</a>
                </p>
                <p style="margin-bottom: 0.5rem;">
                    <strong>论文</strong>: <a href="https://arxiv.org/abs/2505.13211" target="_blank">MAGI-1: Autoregressive
                        Video Generation at Scale (arXiv:2505.13211)</a> |
                    <a href="https://static.magi.world/static/files/MAGI_1.pdf" target="_blank">技术报告 PDF</a>
                </p>
                <p style="margin-bottom: 0;">
                    <strong>开源</strong>: <a href="https://github.com/SandAI-org/MAGI-1" target="_blank">GitHub</a> |
                    <a href="https://huggingface.co/sand-ai/MAGI-1" target="_blank">HuggingFace 权重</a>
                </p>

                <h3 style="margin-top: 1.5rem;">🧠 关键能力：为什么它更像“可流式推理的世界模型”</h3>
                <p style="margin-bottom: 0.75rem;">
                    MAGI-1 的核心设计是把视频生成拆成<strong>chunk（固定长度片段）</strong>并按时间<strong>自回归</strong>生成：模型逐 chunk 预测，并通过“chunk 内整体去噪 +
                    chunk 间因果约束”支持 streaming 与更强的时序一致性。<sup><a href="https://github.com/SandAI-org/MAGI-1"
                            target="_blank">[来源]</a></sup>
                </p>
                <ul>
                    <li><strong>生成范式</strong>: 自回归 chunk-by-chunk；官方描述每个 chunk 为 24 帧，并允许最多 4 个 chunk 并发去噪以提高吞吐。
                        <sup><a href="https://github.com/SandAI-org/MAGI-1" target="_blank">[来源]</a></sup></li>
                    <li><strong>Tokenizer/表示</strong>: Transformer-based VAE，8× 空间压缩、4× 时间压缩。
                        <sup><a href="https://github.com/SandAI-org/MAGI-1" target="_blank">[来源]</a></sup></li>
                    <li><strong>任务覆盖</strong>: 支持 <code>t2v</code>/<code>i2v</code>/<code>v2v</code>（仓库给出参数与示例脚本）。
                        <sup><a href="https://github.com/SandAI-org/MAGI-1" target="_blank">[来源]</a></sup></li>
                    <li><strong>可控性</strong>: 支持 chunk-wise prompting（分段提示词），用于长时合成、平滑转场、细粒度文本控制。
                        <sup><a href="https://github.com/SandAI-org/MAGI-1" target="_blank">[来源]</a></sup></li>
                </ul>

                <h3>⚙️ 落地工程：给想复现/部署的人看的</h3>
                <ul>
                    <li><strong>一键环境</strong>: 官方提供 Docker 镜像与启动命令，降低“能跑起来”的门槛。
                        <sup><a href="https://github.com/SandAI-org/MAGI-1" target="_blank">[来源]</a></sup></li>
                    <li><strong>硬件建议</strong>: 24B 推荐 H100/H800×8；4.5B 可在单张 RTX 4090 运行（并给出 distill / fp8 quant 变体）。
                        <sup><a href="https://github.com/SandAI-org/MAGI-1" target="_blank">[来源]</a></sup></li>
                </ul>
            </div>

            <div class="commentary">
                <div class="commentary-title">🧠 ANTIGRAVITY'S COMMENTARY</div>
                <p>如果你把“世界模型”当成一个<strong>可持续运行的系统</strong>，Magi-1 值得反复读：它把 chunk、并行、硬件预算、推理栈写得足够透明。相比只展示 demo 的闭源产品，
                    Magi-1 的意义更像“教你怎么做一台机器”。</p>
            </div>
        </section>

        <section id="moki">
            <h2>2.5 美图：MOKI —— “我用AI做短片”（工作流优先）</h2>

            <div class="card">
                <h3>🔗 来源与入口（可追溯）</h3>
                <p style="margin-bottom: 0.5rem;">
                    <strong>产品</strong>: <a href="https://www.moki.cn/" target="_blank">moki.cn</a>
                </p>
                <p style="margin-bottom: 0;">
                    <strong>发布与功能描述</strong>: <a href="https://www.ithome.com/0/774/815.htm"
                        target="_blank">IT之家：美图公司推出 AI 短片创作工具 MOKI</a>
                </p>

                <h3 style="margin-top: 1.5rem;">🧠 核心思路：把“不可控的连续生成”拆成“可控的离散工序”</h3>
                <p style="margin-bottom: 0.75rem;">
                    如果你用过文生视频工具做“短片”，会遇到两个硬伤：<strong>叙事节奏不稳</strong>（镜头语言随机）与<strong>成本不可控</strong>（多次重试）。MOKI 的设计取向更像传统影视制作：先把脚本、风格、角色定下来，然后<strong>生成分镜</strong>，把分镜转换为视频素材，再用智能剪辑、AI 配乐、AI 音效、自动字幕完成成片。
                    <sup><a href="https://www.ithome.com/0/774/815.htm" target="_blank">[来源]</a></sup>
                </p>

                <h3>⚙️ 工作流拆解（按公开信息，尽量工程化表达）</h3>
                <ol>
                    <li><strong>Pre-production（前期设定）</strong>：脚本/视觉风格/角色设定（用于统一“世界观”和角色一致性）。<sup><a
                                href="https://www.ithome.com/0/774/815.htm" target="_blank">[来源]</a></sup></li>
                    <li><strong>Storyboard（分镜生成）</strong>：AI 自动生成分镜图（相当于先把镜头列表确定）。<sup><a
                                href="https://www.ithome.com/0/774/815.htm" target="_blank">[来源]</a></sup></li>
                    <li><strong>Materialize（素材化）</strong>：将分镜图转为视频素材（把“图→视频片段”变成可批处理任务）。<sup><a
                                href="https://www.ithome.com/0/774/815.htm" target="_blank">[来源]</a></sup></li>
                    <li><strong>Post-production（后期串联）</strong>：智能剪辑 + AI 配乐/音效 + 自动字幕，完成成片。<sup><a
                                href="https://www.ithome.com/0/774/815.htm" target="_blank">[来源]</a></sup></li>
                </ol>

                <h3>✅ 适用场景（基于“工作流优先”的产品逻辑）</h3>
                <ul>
                    <li><strong>短剧/广告/品牌内容</strong>: 需要稳定镜头语言与成本预算的场景（先分镜，后生成）。</li>
                    <li><strong>企业内容团队</strong>: 更像“素材生产线”，而不是一次性 demo。</li>
                </ul>
            </div>

            <div class="commentary">
                <div class="commentary-title">🧠 ANTIGRAVITY'S COMMENTARY</div>
                <p>判断 MOKI 是否“真能打”，我会盯两个指标：<strong>（1）分镜可编辑性</strong>（能否像真正的分镜脚本那样局部改镜头而不重做全片）与<strong>（2）素材资产复用</strong>（同一角色/风格能否跨项目复用）。
                    这两点决定它是“短视频玩具”还是“生产工具”。</p>
            </div>
        </section>

        <section id="kling">
            <h2>2.6 快手：可灵 AI（Kling AI）—— “能力可组合”的视频生成平台</h2>

            <div class="card">
                <h3>🔗 入口与来源（可追溯）</h3>
                <p style="margin-bottom: 0.5rem;">
                    <strong>产品</strong>: <a href="https://klingai.kuaishou.com/" target="_blank">klingai.kuaishou.com</a> |
                    <a href="https://klingai.com/cn/" target="_blank">klingai.com</a>
                </p>
                <p style="margin-bottom: 0;">
                    <strong>公司</strong>: <a href="https://ir.kuaishou.com/" target="_blank">Kuaishou Technology IR</a>
                </p>

                <h3 style="margin-top: 1.5rem;">🧠 观察框架：从“模型”到“能力层”</h3>
                <p style="margin-bottom: 0.75rem;">
                    Kling 的产品形态非常“工程化”：官网把能力拆成一组可以被调用的模块，而不是只给一个 T2V 模型入口。这意味着它更像一套<strong>视频生成能力层（capability
                        layer）</strong>，适合嵌入到编辑器/电商/营销/社交等不同产品里。<sup><a href="https://klingai.com/cn/"
                            target="_blank">[来源]</a></sup>
                </p>

                <h3>🧩 功能面拆解（官网功能列表 → 对应的系统含义）</h3>
                <table>
                    <thead>
                        <tr>
                            <th>官网能力</th>
                            <th>对系统/工程意味着什么</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>Text to Video</strong> <sup><a href="https://klingai.com/cn/"
                                        target="_blank">[来源]</a></sup></td>
                            <td>基础入口；关键 KPI 是可控性与一致性，而不只是画质。</td>
                        </tr>
                        <tr>
                            <td><strong>Image to Video</strong> <sup><a href="https://klingai.com/cn/"
                                        target="_blank">[来源]</a></sup></td>
                            <td>“参考图锁定”路线（人物/商品一致性），通常比纯 T2V 更靠近商用素材生产。</td>
                        </tr>
                        <tr>
                            <td><strong>Video Extension</strong> <sup><a href="https://klingai.com/cn/"
                                        target="_blank">[来源]</a></sup></td>
                            <td>视频续写/扩展；本质是 prefix-conditioned continuation，对“长视频/交互”是硬前置能力。</td>
                        </tr>
                        <tr>
                            <td><strong>Lip-Sync</strong> <sup><a href="https://klingai.com/cn/"
                                        target="_blank">[来源]</a></sup></td>
                            <td>音频条件 + 人脸/身份保持；是短剧/口播/营销视频的刚需模块，工程上意味着音视频对齐评测与身份一致性约束。</td>
                        </tr>
                        <tr>
                            <td><strong>Video Effects</strong> <sup><a href="https://klingai.com/cn/"
                                        target="_blank">[来源]</a></sup></td>
                            <td>更偏模板/玩法；更容易规模化与产品化（但也更容易陷入“滤镜化”竞争）。</td>
                        </tr>
                        <tr>
                            <td><strong>Multi-Image / Multi-Elements</strong> <sup><a href="https://klingai.com/cn/"
                                        target="_blank">[来源]</a></sup></td>
                            <td>多参考/多元素组合，强调可组合输入。这通常意味着更复杂的 conditioning 设计与更明确的失败模式（元素丢失/身份漂移）。</td>
                        </tr>
                    </tbody>
                </table>

                <h3>⚙️ 开发者平台信号</h3>
                <p style="margin-bottom: 0;">
                    官网明确出现“Video V2.5 Turbo model API”等升级提示，说明其对外接口在持续迭代，值得在后续调研里跟踪 API 的输入约束（时长/分辨率/参考图数量）与稳定性。
                    <sup><a href="https://klingai.com/cn/" target="_blank">[来源]</a></sup>
                </p>
            </div>

            <div class="commentary">
                <div class="commentary-title">🧠 ANTIGRAVITY'S COMMENTARY</div>
                <p>做“可灵”的正确打开方式不是把它当成一个模型，而是把它当成<strong>一组可拼装的能力模块</strong>：续写解决长时，口型解决口播，Multi-Image/Multi-Elements 解决一致性与组合，特效解决玩法与增长。
                    真正的差距会出现在：这些模块能否在统一的风格/角色资产上协同工作，而不是各自为政。</p>
            </div>
        </section>

        <section id="jimeng">
            <h2>2.7 抖音 / 字节跳动：即梦 AI（Dreamina）—— “工具 + 社区 + 同款”的创作闭环</h2>

            <div class="card">
                <h3>🔗 来源与入口（可追溯）</h3>
                <p style="margin-bottom: 0.5rem;">
                    <strong>官网</strong>: <a href="https://jimeng.jianying.com/" target="_blank">即梦AI - 即刻造梦</a>
                </p>
                <p style="margin-bottom: 0;">
                    <strong>App Store</strong>: <a
                        href="https://apps.apple.com/cn/app/%E5%8D%B3%E6%A2%A6ai-%E6%8A%96%E9%9F%B3%E6%97%97%E4%B8%8Bai%E5%9B%BE%E7%89%87%E5%92%8C%E8%A7%86%E9%A2%91%E5%B7%A5%E5%85%B7/id6503676563"
                        target="_blank">即梦AI - 抖音旗下AI图片和视频工具</a> |
                    <strong>公司</strong>: <a href="https://www.bytedance.com/" target="_blank">ByteDance</a>
                </p>

                <h3 style="margin-top: 1.5rem;">🧠 产品逻辑：把生成能力变成“可增长的内容网络”</h3>
                <p style="margin-bottom: 0.75rem;">
                    即梦的官方描述非常明确：它不仅提供 AI 图片/视频生成，还提供<strong>探索页浏览作品、链接分享、提示词同款复用</strong>等社区机制。对 AIGC 来说，这意味着增长不只靠“更强模型”，还靠“更强的作品传播与复刻机制”。
                    <sup><a href="https://apps.apple.com/cn/app/%E5%8D%B3%E6%A2%A6ai-%E6%8A%96%E9%9F%B3%E6%97%97%E4%B8%8Bai%E5%9B%BE%E7%89%87%E5%92%8C%E8%A7%86%E9%A2%91%E5%B7%A5%E5%85%B7/id6503676563"
                            target="_blank">[来源]</a></sup>
                </p>

                <h3>⚙️ 能力拆解（来自官方文案）</h3>
                <ul>
                    <li><strong>AI 图片创作</strong>：用自然语言生成图片，并提供编辑能力以继续迭代。<sup><a
                                href="https://apps.apple.com/cn/app/%E5%8D%B3%E6%A2%A6ai-%E6%8A%96%E9%9F%B3%E6%97%97%E4%B8%8Bai%E5%9B%BE%E7%89%87%E5%92%8C%E8%A7%86%E9%A2%91%E5%B7%A5%E5%85%B7/id6503676563"
                                target="_blank">[来源]</a></sup></li>
                    <li><strong>视频创作</strong>：输入想法生成视频，并鼓励“多轮尝试”获得满意结果（这是产品层面对不可控性的现实回应）。<sup><a
                                href="https://apps.apple.com/cn/app/%E5%8D%B3%E6%A2%A6ai-%E6%8A%96%E9%9F%B3%E6%97%97%E4%B8%8Bai%E5%9B%BE%E7%89%87%E5%92%8C%E8%A7%86%E9%A2%91%E5%B7%A5%E5%85%B7/id6503676563"
                                target="_blank">[来源]</a></sup></li>
                    <li><strong>探索与同款</strong>：浏览他人作品、复用提示词生成同款（提示词复刻是社区“教学”的核心机制）。<sup><a
                                href="https://apps.apple.com/cn/app/%E5%8D%B3%E6%A2%A6ai-%E6%8A%96%E9%9F%B3%E6%97%97%E4%B8%8Bai%E5%9B%BE%E7%89%87%E5%92%8C%E8%A7%86%E9%A2%91%E5%B7%A5%E5%85%B7/id6503676563"
                                target="_blank">[来源]</a></sup></li>
                    <li><strong>会员权益</strong>：官方提及“视频延长、去水印”等权益，说明其商业化锚点与视频生成的系统约束（时长/队列/输出）强相关。<sup><a
                                href="https://apps.apple.com/cn/app/%E5%8D%B3%E6%A2%A6ai-%E6%8A%96%E9%9F%B3%E6%97%97%E4%B8%8Bai%E5%9B%BE%E7%89%87%E5%92%8C%E8%A7%86%E9%A2%91%E5%B7%A5%E5%85%B7/id6503676563"
                                target="_blank">[来源]</a></sup></li>
                </ul>
            </div>

            <div class="commentary">
                <div class="commentary-title">🧠 ANTIGRAVITY'S COMMENTARY</div>
                <p>即梦这类“社区型 AIGC”最怕两件事：<strong>（1）同款不可复现</strong>（提示词/参数不透明导致只能看不能学），<strong>（2）视频生成的等待与失败成本过高</strong>（队列/失败率拉低留存）。
                    后续我会把它和 PixVerse/MOKI 的“工作流型产品”做一张对比表：谁更像内容社区，谁更像生产线。</p>
            </div>
        </section>

        <section id="huiwa">
            <h2>2.8 阿里巴巴：绘蛙（Huiwa）—— 电商内容“图+文案+模特资产”的生产线</h2>

            <div class="card">
                <h3>🔗 入口与来源（可追溯）</h3>
                <p style="margin-bottom: 0.5rem;">
                    <strong>产品</strong>: <a href="https://www.ihuiwa.com/" target="_blank">ihuiwa.com</a>
                </p>
                <p style="margin-bottom: 0;">
                    <strong>发布/介绍（阿里系渠道）</strong>: <a href="https://developer.aliyun.com/article/1452351"
                        target="_blank">阿里上线AI电商工具“绘蛙” - 阿里云开发者社区</a>
                </p>

                <h3 style="margin-top: 1.5rem;">🧠 核心逻辑：电商不是“生成一张图”，而是“批量生产一套可投放素材”</h3>
                <p style="margin-bottom: 0.75rem;">
                    绘蛙把目标写得非常直白：为电商场景提供图片与文案解决方案，核心价值是<strong>降低拍摄成本</strong>与<strong>规模化产出营销素材</strong>。相比通用视频/图片模型，绘蛙更强调“可复用资产”（商品模型/模特模型）与“可直接投放的内容形态”（主图、小红书图、种草文案、口播文案）。
                    <sup><a href="https://www.ihuiwa.com/" target="_blank">[来源]</a></sup>
                    <sup><a href="https://developer.aliyun.com/article/1452351" target="_blank">[来源]</a></sup>
                </p>

                <h3>⚙️ 能力拆解（从官网描述整理为“电商内容流水线”）</h3>
                <ol>
                    <li><strong>资产准备</strong>：提供海量虚拟模特；支持训练自己的商品模型/模特模型（用于一致性与复用）。<sup><a
                                href="https://www.ihuiwa.com/" target="_blank">[来源]</a></sup></li>
                    <li><strong>图像生产</strong>：生成电商商品主图、跨境主图、小红书图片等营销图，并支持换装/换背景等变体生产。<sup><a
                                href="https://www.ihuiwa.com/" target="_blank">[来源]</a></sup></li>
                    <li><strong>文案生产</strong>：生成种草文案、穿搭文案等（内容电商化）。<sup><a
                                href="https://www.ihuiwa.com/" target="_blank">[来源]</a></sup></li>
                    <li><strong>视频脚本</strong>：生成视频口播文案（为短视频/直播带货内容供给做准备）。<sup><a
                                href="https://www.ihuiwa.com/" target="_blank">[来源]</a></sup></li>
                    <li><strong>后期工具箱</strong>：去水印、智能消除、换脸、高清修复等一键工具（覆盖电商常见修图需求）。<sup><a
                                href="https://www.ihuiwa.com/" target="_blank">[来源]</a></sup></li>
                </ol>

                <h3>✅ 适用场景（更偏“电商营销”而非“影视创作”）</h3>
                <ul>
                    <li><strong>电商上新</strong>：同一商品快速生成多风格、多背景、多卖点素材。</li>
                    <li><strong>达人种草</strong>：图 + 文案一起产出，降低内容团队成本。</li>
                    <li><strong>跨境电商</strong>：多语种/多平台素材变体（需要进一步核实具体支持范围）。</li>
                </ul>
            </div>

            <div class="commentary">
                <div class="commentary-title">🧠 ANTIGRAVITY'S COMMENTARY</div>
                <p>绘蛙的胜负手在“资产化”：只要商品/模特模型训练入口可用、质量稳定，它就能形成电商团队最需要的<strong>素材版本管理 + 复用</strong>能力。下一步值得深挖：是否存在“品牌资产库/素材版本回归/投放 AB”的产品化闭环（这决定它是工具还是平台）。</p>
            </div>
        </section>

        <section id="hailuo">
            <h2>2.9 MiniMax：海螺视频（Hailuo AI）—— 把“模型/分辨率/时长/成本”做成可选项</h2>

            <div class="card">
                <h3>🔗 入口与来源（可追溯）</h3>
                <p style="margin-bottom: 0.5rem;">
                    <strong>产品</strong>: <a href="https://hailuoai.com/" target="_blank">hailuoai.com</a>
                </p>
                <p style="margin-bottom: 0;">
                    <strong>公司</strong>: <a href="https://www.minimaxi.com/" target="_blank">MiniMax（minimaxi.com）</a>
                </p>

                <h3 style="margin-top: 1.5rem;">🧠 为什么海螺值得看：它把“生成”变成“可配置的商品”</h3>
                <p style="margin-bottom: 0.75rem;">
                    海螺官网页面信息中出现 Hailuo 2.3 / Hailuo 2.3-Fast 等模型，并提供分辨率（768p/1080p）与时长（6s/10s）等选项，同时把成本做成明确梯度——这是一种非常典型的“产品化大模型”路径：把能力切分成可售卖的规格。
                    <sup><a href="https://hailuoai.com/" target="_blank">[来源]</a></sup>
                </p>

                <h3>📊 规格与成本（从官网配置项整理成表）</h3>
                <table>
                    <thead>
                        <tr>
                            <th>分辨率</th>
                            <th>时长</th>
                            <th>示例成本（积分）</th>
                        </tr>
                    </thead>
                    <tbody>
                        <tr>
                            <td><strong>768p</strong></td>
                            <td>6s</td>
                            <td>25</td>
                        </tr>
                        <tr>
                            <td><strong>768p</strong></td>
                            <td>10s</td>
                            <td>50</td>
                        </tr>
                        <tr>
                            <td><strong>1080p</strong></td>
                            <td>6s</td>
                            <td>80</td>
                        </tr>
                    </tbody>
                </table>
                <p style="font-size: 0.9rem; color: var(--text-secondary); margin-top: -0.5rem;">
                    <strong>说明</strong>: 以上为官网页面配置项中某模型（示例项）的分辨率/时长/成本组合，具体模型与权益可能随版本迭代变化。来源：<a href="https://hailuoai.com/"
                        target="_blank">hailuoai.com</a>
                </p>

                <h3>🧩 闲时模式“无限生成”：典型的队列与时段策略</h3>
                <p style="margin-bottom: 0;">
                    官网出现“无限图像和视频生成”及“用完积分后仍可在闲时模式使用 Hailuo 系列模型无限生成”的文案。这通常意味着平台用队列/时段调度把长尾需求吸纳进来，从而降低边际成本并提高留存。
                    <sup><a href="https://hailuoai.com/" target="_blank">[来源]</a></sup>
                </p>
            </div>

            <div class="commentary">
                <div class="commentary-title">🧠 ANTIGRAVITY'S COMMENTARY</div>
                <p>海螺是一个“产品经理视角很强”的样本：它把模型、分辨率、时长与成本组合成一套<strong>可选型</strong>系统，再用“闲时无限生成”做差异化。后续最值得深挖的是：2.3 与 2.3-Fast 的真实速度差异、质量退化曲线、以及是否提供更强的可控接口（参考图、镜头、运动控制）。</p>
            </div>
        </section>

        <section id="decision">
            <h2>📊 选型决策：我该用谁？</h2>
            <table>
                <thead>
                    <tr>
                        <th>你的需求</th>
                        <th>推荐模型</th>
                        <th>原因</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>我要做 3A 游戏的宣传片 (PV)</strong></td>
                        <td><strong><a href="https://www.vidu.studio/" target="_blank">Vidu Q2</a></strong></td>
                        <td>画质最顶，参考图控制最稳，素材不崩。</td>
                    </tr>
                    <tr>
                        <td><strong>我要做互动视频 / 短剧 APP</strong></td>
                        <td><strong><a href="https://pixverse.ai/" target="_blank">PixVerse V5.5</a></strong></td>
                        <td>自动分镜 + 音效，极大降低内容生产成本。</td>
                    </tr>
                    <tr>
                        <td><strong>我要做下一代 AI 游戏 / 模拟器</strong></td>
                        <td><strong><a
                                    href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/"
                                    target="_blank">Genie 3 (复现)</a></strong></td>
                        <td>只有它能提供真正的实时交互和物理模拟。</td>
                    </tr>
                </tbody>
            </table>
        </section>

        <nav class="chapter-nav">
            <a href="01_industry.html" class="chapter-nav-link prev">
                <span class="chapter-nav-label">← 上一章</span>
                <span class="chapter-nav-title">01. 行业全景 (Landscape)</span>
            </a>
            <a href="03_architecture.html" class="chapter-nav-link next">
                <span class="chapter-nav-label">下一章 →</span>
                <span class="chapter-nav-title">03. 技术架构 (Architecture)</span>
            </a>
        </nav>
    </main>
</body>

</html>