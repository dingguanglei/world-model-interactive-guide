<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>03. 技术架构 - World Model Guide</title>
    <link rel="stylesheet" href="css/style.css">
    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({ startOnLoad: true, theme: 'default' });</script>
</head>

<body>
    <div class="aurora-bg">
        <div class="aurora-blob aurora-blob-1"></div>
        <div class="aurora-blob aurora-blob-2"></div>
        <div class="aurora-blob aurora-blob-3"></div>
        <div class="aurora-blob aurora-blob-4"></div>
    </div>
    <nav class="sidebar">
        <a href="index.html" class="brand">🚀 World Model Guide</a>
        <ul class="nav-links">
            <li class="nav-item"><a href="index.html" class="nav-link">00. 概览 (Overview)</a></li>
            <li class="nav-item"><a href="01_industry.html" class="nav-link">01. 行业全景 (Landscape)</a></li>
            <li class="nav-item"><a href="02_product.html" class="nav-link">02. 产品深度 (Deep Dive)</a></li>
            <li class="nav-item"><a href="03_architecture.html" class="nav-link active">03. 技术架构 (Architecture)</a></li>
            <li class="nav-item"><a href="04_data.html" class="nav-link">04. 数据工程 (Data Bible)</a></li>
            <li class="nav-item"><a href="05_roadmap.html" class="nav-link">05. 落地路线 (Roadmap)</a></li>
            <li class="nav-item"><a href="06_companies.html" class="nav-link">06. 公司调研 (Companies)</a></li>
            <li class="nav-item"><a href="07_paper_tracker.html" class="nav-link">07. 论文追踪 (Papers)</a></li>
            <li class="nav-item"><a href="08_community.html" class="nav-link">08. 社区动态 (Community)</a></li>
            <li class="nav-item"><a href="references.html" class="nav-link">附录：参考文献 (Refs)</a></li>
        </ul>
    </nav>

    <main class="main-content">
        <h1>03. 核心技术架构与关键论文 (Architecture)</h1>
        <p>本章我们拆解"如何让模型听懂键盘指令"以及"如何跑得比显卡还快"。我们将深入探讨 Tokenizer、Streaming Inference 和 Action Injection 的底层机制。</p>

        <section>
            <h2>3.1 架构对比：视频模型 vs 游戏模型</h2>
            <p>从技术角度看，<a href="https://openai.com/sora" target="_blank">Sora</a> 这类 Video Gen 和 <a
                    href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/"
                    target="_blank">Genie 3</a> 这类 Game Gen 存在本质的的区别。</p>
            <table>
                <thead>
                    <tr>
                        <th>组件</th>
                        <th>传统视频模型 (Video DiT)</th>
                        <th>交互世界模型 (Game DiT)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>输入 Condition</strong></td>
                        <td>Text Prompt, First Image</td>
                        <td><strong>Action Token (WASD)</strong>, Past Latent</td>
                    </tr>
                    <tr>
                        <td><strong>Tokenizer</strong></td>
                        <td>2D VAE (SDXL Style)</td>
                        <td><strong>3D Causal VAE (Magvit-v2 Style)</strong></td>
                    </tr>
                    <tr>
                        <td><strong>Attention</strong></td>
                        <td>Spatial-Temporal (Full)</td>
                        <td><strong>Causal Attention</strong> (Masked Future)</td>
                    </tr>
                    <tr>
                        <td><strong>推理模式</strong></td>
                        <td>Parallel Decoding (一次出全片)</td>
                        <td><strong>Autoregressive Streaming</strong> (逐帧出片)</td>
                    </tr>
                </tbody>
            </table>

            <div class="commentary">
                <div class="commentary-title">🧠 AntiGravity's Commentary</div>
                <p>很多人以为把 Sora 的推理做快点就能变成游戏引擎，这是<strong>根本性的误判</strong>。Video Gen 的本质是
                    "In-painting"（填补像素），它默认可以看到未来的帧来推现在的帧。而 Game Gen 的本质是 "State
                    Transition"（状态转移），你永远不知道玩家下一秒会按什么键。架构上必须强制 Causal Masking，否则模型会"偷看"未来，导致无法响应玩家当前的突发操作。</p>
            </div>
        </section>

        <section>
            <h2>3.2 深度解剖 1：Magvit-v2 3D Tokenizer</h2>
            <p>为什么 <a href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/"
                    target="_blank">Genie 3</a> 能做到实时生成？关键在于它把视频压缩成了极其紧凑的 <strong>Discrete Tokens</strong>。</p>

            <pre class="mermaid">
flowchart LR
    subgraph Input
        Video[Raw 16 Frames]
    end
    
    subgraph Encoder [3D Causal CNN]
        C3D1[Conv3D + Time Padding]
        C3D2[ResBlock 3D]
    end
    
    subgraph Quantizer [LFQ]
        Vector[Latent] -->|Sign > 0| Binary[1]
        Vector -->|Sign < 0| Binary2[0]
        Binary -->|Binary2Int| Index[Token ID]
    end
    
    Input --> Encoder --> Quantizer
            </pre>

            <div class="commentary">
                <div class="commentary-title">🧠 AntiGravity's Commentary</div>
                <p>Magvit-v2 的真正的突破不是 3D CNN，而是 <strong>LFQ (Lookup-Free Quantization)</strong>。以前的 VQ-VAE
                    需要在几十万的词表里"查字典"(Nearest Neighbor Search)，慢得离谱。LFQ 直接看向量符号是正还是负 ($z > 0 \to 1, z < 0 \to
                        0$)，把"查字典"变成了"位运算"。这是 Genie 3 能跑 24fps 的隐形功臣。</p>
            </div>
        </section>

        <section>
            <h2>3.3 深度解剖 2：实时流式推理工程 (Real-time Streaming)</h2>
            <p>光有模型不够，怎么让 Web 端的玩家感觉不到延迟？</p>

            <h3>1. KV Cache 极致优化</h3>
            <p>在 DiT 中，每一帧生成都依赖历史帧。如果你每次都重算历史帧的 Attention，延迟是 $O(N^2)$。必须引入 <strong>KV Cache (Key-Value
                    Cache)</strong>，把过去几秒的显存特征存下来。</p>
            <ul>
                <li><strong>Paged KV Cache</strong>: 像操作系统管理内存一样管理显存，防止碎片化。</li>
                <li><strong>Window Attention</strong>: 只看最近 5秒（Sliding Window），丢弃太久远的记忆，换取 $O(1)$ 的推理复杂度。</li>
            </ul>

            <h3>2. 网络传输协议选择</h3>
            <table>
                <thead>
                    <tr>
                        <th>协议</th>
                        <th>适用场景</th>
                        <th>延迟</th>
                        <th>评价</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>HLS/DASH</strong></td>
                        <td>视频网站</td>
                        <td>5-30s</td>
                        <td><strong>不可用</strong>。等你看到画面，角色已经死了一万次了。</td>
                    </tr>
                    <tr>
                        <td><strong>WebSocket</strong></td>
                        <td>聊天室</td>
                        <td>100-500ms</td>
                        <td><strong>勉强可用</strong>。传输 Base64 图片可以，但带宽爆炸。</td>
                    </tr>
                    <tr>
                        <td><strong>WebRTC</strong></td>
                        <td>云游戏/Zoom</td>
                        <td><strong>&lt; 50ms</strong></td>
                        <td><strong>唯一解</strong>。UDP 协议，允许丢包（画面花屏总比卡死强）。</td>
                    </tr>
                </tbody>
            </table>

            <pre class="mermaid">
sequenceDiagram
    participant Player
    participant Edge_Server
    participant GPU_Cluster
    
    Player->>Edge_Server: Action 'W' (UDP)
    Edge_Server->>GPU_Cluster: Action Embedding
    
    par Parallel Processing
        GPU_Cluster->>GPU_Cluster: DiT Inference (Next Latent)
        GPU_Cluster->>GPU_Cluster: VAE Decode (Next Frame)
    end
    
    GPU_Cluster->>Edge_Server: Raw RGB Frame
    Edge_Server->>Player: H.264 Stream (WebRTC)
            </pre>

            <div class="commentary">
                <div class="commentary-title">🧠 AntiGravity's Commentary</div>
                <p>不要尝试自己用 Python Flask 写视频流。工程落地的关键是 <strong>WebRTC + GPU Encode</strong>。推理出来的 Frame 不要拷回 CPU，直接在显卡里用
                    NVENC 编码成 H.264，然后直接发包。做到 <strong>Zero-Copy</strong>，否则 PCIe 带宽会成为瓶颈。</p>
            </div>
        </section>

        <section>
            <h2>3.4 深度解剖 3：动作注入机制 (Action Injection)</h2>
            <p><a href="https://arxiv.org/abs/2508.13009" target="_blank">Matrix-Game 2.0</a> 是如何让 DiT 理解 "W 键" 的？</p>
            <ol>
                <li><strong>分桶 (Binning)</strong>: 把鼠标的 <code>Delta X=120</code> 变成 <code>Mouse_Right_Fast</code> 这样一个离散
                    Token。</li>
                <li><strong>Cross-Attention</strong>: 把动作 Token 当作 Prompt，注入到每一个像素的生成过程中。</li>
            </ol>

            <div class="commentary">
                <div class="commentary-title">🧠 AntiGravity's Commentary</div>
                <p>目前的 Action Injection 还是"硬注入"。未来的方向是 <strong>Latent Action Learning</strong> (如 Genie
                    1)，让模型自己从视频里学会什么是"跳"，而不是人工标注 "Spacebar"。这样才能利用 YouTube 上无限的无标注视频数据。</p>
            </div>
        </section>

        <section>
            <h2>3.5 关键论文详情 (Paper Deep Dive)</h2>
            <p>详见附录 <a href="references.html">参考文献页</a>，那里有我对每一篇论文的"通俗翻译"和"毒舌点评"。</p>
        </section>

    </main>
</body>

</html>