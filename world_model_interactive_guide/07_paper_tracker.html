<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>07. 论文追踪 - World Model Guide</title>
    <link rel="stylesheet" href="css/style.css">
    <style>
        .paper-card {
            background: white;
            border: 1px solid var(--border);
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin: 1.5rem 0;
            box-shadow: var(--card-shadow);
        }

        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .paper-title {
            font-size: 1.2rem;
            font-weight: 700;
            color: var(--text-primary);
            margin-bottom: 0.5rem;
        }

        .paper-title a {
            color: var(--accent);
            text-decoration: none;
        }

        .paper-title a:hover {
            text-decoration: underline;
        }

        .paper-meta {
            font-size: 0.85rem;
            color: var(--text-secondary);
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
            margin-bottom: 1rem;
        }

        .paper-tag {
            background: #e0f2fe;
            color: #0369a1;
            padding: 0.2rem 0.6rem;
            border-radius: 1rem;
            font-size: 0.75rem;
            font-weight: 600;
        }

        .paper-tag.github {
            background: #f0fdf4;
            color: #15803d;
        }

        .paper-tag.new {
            background: #fef3c7;
            color: #b45309;
        }

        .paper-tag a {
            color: inherit;
            text-decoration: none;
        }

        .paper-tag a:hover {
            text-decoration: underline;
        }

        .dual-view {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 1.5rem;
            margin: 1rem 0;
        }

        .layman-view {
            background: #f0fae6;
            padding: 1rem;
            border-radius: 0.5rem;
            border-left: 4px solid #22c55e;
        }

        .pro-view {
            background: #fff1f2;
            padding: 1rem;
            border-radius: 0.5rem;
            border-left: 4px solid #e11d48;
        }

        .view-title {
            font-weight: 700;
            font-size: 0.9rem;
            margin-bottom: 0.5rem;
        }

        .layman-view .view-title {
            color: #166534;
        }

        .pro-view .view-title {
            color: #9f1239;
        }

        .paper-sections {
            display: flex;
            flex-direction: column;
            gap: 1rem;
            margin: 1rem 0;
        }

        .paper-figure {
            margin: 0.75rem 0 0;
            background: rgba(255, 255, 255, 0.65);
            border: 1px solid var(--border);
            border-radius: 0.5rem;
            padding: 0.75rem;
        }

        .paper-figure figcaption {
            font-size: 0.85rem;
            color: var(--text-secondary);
            margin-top: 0.5rem;
        }

        .last-updated {
            background: #fef3c7;
            border: 1px solid #fcd34d;
            padding: 1rem;
            border-radius: 0.5rem;
            font-size: 0.9rem;
            margin-bottom: 2rem;
        }

        @media (max-width: 768px) {
            .dual-view {
                grid-template-columns: 1fr;
            }
        }
    </style>
</head>

<body>
    <div class="aurora-bg">
        <div class="aurora-blob aurora-blob-1"></div>
        <div class="aurora-blob aurora-blob-2"></div>
        <div class="aurora-blob aurora-blob-3"></div>
        <div class="aurora-blob aurora-blob-4"></div>
    </div>
    <nav class="sidebar">
        <a href="index.html" class="brand">🚀 World Model Guide</a>
        <ul class="nav-links">
            <li class="nav-item"><a href="index.html" class="nav-link">00. 概览 (Overview)</a></li>
            <li class="nav-item"><a href="01_industry.html" class="nav-link">01. 行业全景 (Landscape)</a></li>
            <li class="nav-item"><a href="02_product.html" class="nav-link">02. 产品深度 (Deep Dive)</a></li>
            <li class="nav-item"><a href="03_architecture.html" class="nav-link">03. 技术架构 (Architecture)</a></li>
            <li class="nav-item"><a href="04_data.html" class="nav-link">04. 数据工程 (Data Bible)</a></li>
            <li class="nav-item"><a href="05_roadmap.html" class="nav-link">05. 落地路线 (Roadmap)</a></li>
            <li class="nav-item"><a href="06_companies.html" class="nav-link">06. 公司调研 (Companies)</a></li>
            <li class="nav-item"><a href="07_paper_tracker.html" class="nav-link active">07. 论文追踪 (Papers)</a></li>
            <li class="nav-item"><a href="08_community.html" class="nav-link">08. 社区动态 (Community)</a></li>
            <li class="nav-item"><a href="references.html" class="nav-link">附录：参考文献 (Refs)</a></li>
        </ul>
    </nav>

    <main class="main-content">
        <h1>07. 最新论文追踪 (Paper Tracker)</h1>
        <p>本页追踪 arXiv/GitHub 上关于<strong>世界模型
                (非具身智能)</strong>、<strong>交互视频生成</strong>、<strong>游戏模拟</strong>的最新研究。按发布日期从新到旧排列。</p>

        <div class="last-updated">
            ⏰ <strong>最后更新时间</strong>: 2026-01-20 | 本页内容将每日更新「世界模型（非具身）/交互视频生成/神经游戏引擎方向的最新论文与项目」。
        </div>

        <section>
            <h2>📅 2026年1月</h2>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2503.20314" target="_blank">Wan（论文）:
                                Open and Advanced Large-Scale Video Generative Models</a> — Wan 2.2（模型版本）</div>
                        <div class="paper-meta">
                            <span>📅 2026-01-20</span>
                            <span>📄 arXiv:2503.20314</span>
                            <span class="paper-tag github"><a href="https://github.com/Wan-Video/Wan2.2"
                                    target="_blank">GitHub: Wan2.2</a></span>
                            <span class="paper-tag new">NEW</span>
                        </div>
                    </div>
                </div>

                <div class="paper-sections">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>把 Wan 2.2 理解为“更好用的版本”更准确：你更希望它<strong>更快</strong>、<strong>更稳</strong>、<strong>更可控</strong>，而不是只在“更清晰一点”上做文章。对交互/长视频方向来说，“不卡 + 不漂 + 能控”是产品化底线。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解读</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>Problem / Setup</strong>: 大规模视频生成模型的核心矛盾是：高质量通常意味着高算力与长采样链路，而交互/长视频要求低延迟与稳定状态。</li>
                            <li><strong>Method（论文层面）</strong>: 详见 Wan 论文（arXiv:2503.20314）对模型与训练配方的系统描述；模型版本（2.1/2.2）通常体现为工程与推理侧迭代。</li>
                            <li><strong>Evaluation</strong>: 除传统视频质量指标外，建议额外关注长段一致性、控制可复现性与推理速度（端到端 latency）。</li>
                            <li><strong>Limitations</strong>: 若缺少显式状态读写/记忆机制，长交互下仍会出现不可逆漂移；需要系统级失败恢复策略。</li>
                        </ul>
                    </div>
                    <div class="commentary">
                        <div class="commentary-title">🧠 ntiGravity's Commentary</div>
                        <p>对 Wan 这类“强基座视频模型”，我更看重它是否给出<strong>可复现的工程路径</strong>（推理加速、缓存、量化、流式接口）以及能否与“世界状态/记忆”模块对接。否则它更像“更强的摄像机”，而不是“更可玩的世界”。</p>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2503.20314" target="_blank">Wan（论文）:
                                Open and Advanced Large-Scale Video Generative Models</a> — Wan 2.1（模型版本）</div>
                        <div class="paper-meta">
                            <span>📅 2026-01-20</span>
                            <span>📄 arXiv:2503.20314</span>
                            <span class="paper-tag github"><a href="https://github.com/Wan-Video/Wan2.1"
                                    target="_blank">GitHub: Wan2.1</a></span>
                            <span class="paper-tag new">NEW</span>
                        </div>
                    </div>
                </div>

                <div class="paper-sections">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>Wan 2.1 可以理解成“基础可用版本”：质量与基本能力到位，但如果把它直接拿来做长时交互世界模型，很可能会遇到“玩久了就跑偏、越玩越不可控”的老问题。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解读</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>定位</strong>: 作为版本迭代基线，便于对比 2.2 在速度、稳定性、控制接口上的增量。</li>
                            <li><strong>可迁移点</strong>: 基座视频生成的 tokenizer/conditioning/训练 recipe 往往可复用到交互世界模型，但“状态/记忆/可恢复”通常需要额外模块。</li>
                            <li><strong>评估建议</strong>: 做长视频分段一致性评测（跨 chunk 的物体恒常性）与控制误差评测，而不是只看短视频质量。</li>
                        </ul>
                    </div>
                    <div class="commentary">
                        <div class="commentary-title">🧠 ntiGravity's Commentary</div>
                        <p>把 2.1/2.2 当作“版本号玄学”很危险：真正重要的是它们是否带来<strong>可交互门槛</strong>所需的端到端工程指标改善（latency、可复现控制、失败恢复），以及是否暴露出可对接的接口（action/state）。</p>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2601.05232" target="_blank">4D Geometric
                                Control for Video World Models</a></div>
                        <div class="paper-meta">
                            <span>📅 2026-01-09</span>
                            <span>📄 arXiv:2601.05232</span>
                            <span class="paper-tag new">NEW</span>
                        </div>
                    </div>
                </div>

                <div class="paper-sections">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>这篇论文的核心是：用更“像游戏引擎”的方式控制视频。你不是只写一句话让模型自由发挥，而是给出<strong>相机与物体在时间维度上的几何轨迹</strong>（可理解为 4D：3D 空间 + 时间），让生成结果更可控、更可复现。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解读</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>Problem / Setup</strong>: 纯文本控制存在歧义与不可复现；需要一种可组合、可精确约束的控制信号。</li>
                            <li><strong>Method</strong>: 以几何/运动表示统一编码相机与物体动态，将其渲染/投影为可被视频生成模型消费的 conditioning。</li>
                            <li><strong>Why it works</strong>: 将“想要的运动”从语言空间迁移到几何空间，减少模型在动作与相机运动上的自由度，从而提升一致性。</li>
                            <li><strong>Evaluation</strong>: 可控性（轨迹误差）、一致性（跨帧稳定）、以及与文本/参考图的组合控制能力。</li>
                        </ul>
                    </div>
                    <div class="commentary">
                        <div class="commentary-title">🧠 ntiGravity's Commentary</div>
                        <p>我把这类工作视为“世界模型的控制层”——它解决的是<strong>接口问题</strong>：如何把可复现的意图（轨迹/相机）喂给生成器。真正的产品化还需要把这层接口与 state/memory、失败恢复连起来，才能支撑长交互。</p>
                    </div>
                </div>
            </div>
        </section>

        <section>
            <h2>📅 2025年12月</h2>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2512.16093"
                                target="_blank">TurboDiffusion: Accelerating Video Diffusion by 100-200x</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-12-20</span>
                            <span>📄 arXiv:2512.16093</span>
                            <span class="paper-tag github"><a href="https://github.com/thu-ml/TurboDiffusion"
                                    target="_blank">GitHub</a></span>
                        </div>
                    </div>
                </div>

                <div class="paper-sections">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>这篇论文本质是在做“视频扩散的<strong>涡轮增压</strong>”：把原本需要很多步才能生成的视频，压缩到极少步数完成，并尽量保持质量不崩。这类加速是把世界模型推向“可交互”的必经之路。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解读</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>Problem / Setup</strong>: 扩散采样步数与时空注意力带来高延迟，难以满足实时/交互预算。</li>
                            <li><strong>Method</strong>: 通过结构/注意力优化与一致性类蒸馏把采样步数显著压缩，并配合量化降低显存与吞吐瓶颈。</li>
                            <li><strong>Evaluation</strong>: 除质量指标外，更关键是端到端速度（fps/latency）与加速后的一致性损失。</li>
                            <li><strong>Limitations</strong>: 加速往往带来分布外失真与可控性折损，需要与更稳的 conditioning/state 设计配合。</li>
                        </ul>
                    </div>
                    <div class="commentary">
                        <div class="commentary-title">🧠 ntiGravity's Commentary</div>
                        <p>“加速”不是锦上添花，而是交互世界模型的地基。我会优先看它是否提供<strong>可落地的推理栈</strong>（量化/缓存/并行）与清晰的速度-质量曲线，而不是只给一个峰值加速数字。</p>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2512.04040" target="_blank">RELIC:
                                Interactive Video World Model with Long-Horizon Memory</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-12-03</span>
                            <span>📄 arXiv:2512.04040</span>
                        </div>
                    </div>
                </div>

                <div class="paper-sections">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>RELIC 的目标是给“视频世界”加上<strong>长期记忆</strong>：你离开场景再回来，物体不应该随机换位置。对交互世界模型来说，这相当于从“会画”迈向“会记”。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解读</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>Problem / Setup</strong>: 仅靠短上下文/隐式记忆会在长跨度下发生漂移，表现为物体消失、身份错乱、场景重置。</li>
                            <li><strong>Method</strong>: 引入显式 Memory（例如 memory bank / key-value 存储 / 可检索状态），把跨段稳定信息从生成主干中“外置”。</li>
                            <li><strong>Why it works</strong>: 让模型在生成时可以“查档”，而不是完全依赖最近几帧的像素相关性。</li>
                            <li><strong>Evaluation</strong>: 物体恒常性、长段一致性、回访一致性（revisit consistency）、以及交互动作下的状态可读写性。</li>
                            <li><strong>Limitations</strong>: 显式记忆带来检索误差与写入噪声；需要设计写入策略与冲突解决（同一物体多次观测融合）。</li>
                        </ul>

                        <figure class="paper-figure">
                            <svg viewBox="0 0 900 170" width="100%" height="170" role="img" aria-label="RELIC 显式记忆辅助长时一致性的示意图">
                                <rect x="20" y="35" width="250" height="100" rx="14" fill="#fff1f2" stroke="#fda4af" />
                                <text x="145" y="80" text-anchor="middle" font-size="16" fill="#1e293b">Video Generator</text>
                                <text x="145" y="102" text-anchor="middle" font-size="12" fill="#475569">(diffusion / transformer)</text>

                                <rect x="330" y="20" width="240" height="60" rx="14" fill="#f0fdf4" stroke="#86efac" />
                                <text x="450" y="55" text-anchor="middle" font-size="14" fill="#1e293b">Write: observations → memory</text>

                                <rect x="330" y="95" width="240" height="60" rx="14" fill="#f0fae6" stroke="#86efac" />
                                <text x="450" y="130" text-anchor="middle" font-size="14" fill="#1e293b">Read: retrieve state → condition</text>

                                <rect x="630" y="35" width="250" height="100" rx="14" fill="#dbeafe" stroke="#93c5fd" />
                                <text x="755" y="80" text-anchor="middle" font-size="16" fill="#1e293b">Explicit Memory</text>
                                <text x="755" y="102" text-anchor="middle" font-size="12" fill="#475569">(bank / KV store)</text>

                                <path d="M270 85 L330 50" stroke="#64748b" stroke-width="3" />
                                <path d="M570 50 L630 85" stroke="#64748b" stroke-width="3" />
                                <path d="M270 85 L330 125" stroke="#64748b" stroke-width="3" />
                                <path d="M570 125 L630 85" stroke="#64748b" stroke-width="3" />
                            </svg>
                            <figcaption>示意图：把长期稳定信息外置为可读写的显式记忆，降低“只靠上下文”导致的长时漂移。</figcaption>
                        </figure>
                    </div>
                    <div class="commentary">
                        <div class="commentary-title">🧠 ntiGravity's Commentary</div>
                        <p>“显式记忆”是交互世界模型的分水岭：没有它，长时一致性很难靠堆模型解决。我会重点看 RELIC 的记忆写入/检索策略是否<strong>可扩展、可诊断</strong>（能回答：错在哪一条记忆？如何回滚？）。</p>
                    </div>
                </div>
            </div>
        </section>

        <section>
            <h2>📅 2025年9月</h2>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2509.22622" target="_blank">LongLive:
                                Real-time Interactive Long Video Generation</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-09-26</span>
                            <span>📄 arXiv:2509.22622</span>
                            <span class="paper-tag github"><a href="https://github.com/NVlabs/LongLive"
                                    target="_blank">GitHub</a></span>
                            <span class="paper-tag"><a href="https://nvlabs.github.io/LongLive/" target="_blank">Project</a></span>
                        </div>
                    </div>
                </div>

                <div class="paper-sections">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>它要做“能一直玩”的视频生成：你不断输入操作，模型就持续生成后续画面，并尽量保持世界状态不崩。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解读</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>核心挑战</strong>: 长时交互下的误差累积（drift）与实时延迟预算同时成立。</li>
                            <li><strong>关键能力</strong>: 流式/分块生成、状态缓存与锚点对齐、动作条件注入的稳定接口。</li>
                            <li><strong>评估重点</strong>: 延迟（latency/fps）、长时一致性（回访一致性/物体恒常性）、控制响应（可逆/可恢复）。</li>
                        </ul>
                    </div>
                    <div class="commentary">
                        <div class="commentary-title">🧠 ntiGravity's Commentary</div>
                        <p>LongLive 的真正价值在于把“交互世界模型”的系统问题讲清楚：<strong>延迟、漂移、可控、可恢复</strong>缺一不可。后续建议把它的评测维度抽成表格，对标 Matrix-Game 2.0 与 Genie 3。</p>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://openreview.net/forum?id=worldgym"
                                target="_blank">WorldGym: World Model as an Environment for Policy Evaluation</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-09-19</span>
                            <span>📄 OpenReview (NeurIPS Submission)</span>
                        </div>
                    </div>
                </div>

                <div class="paper-sections">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>WorldGym 把世界模型当作一个“虚拟环境”，让 agent 可以在里面反复练习与评估策略。它更像是在回答：世界模型能不能不仅生成画面，还能当“可用的模拟器”。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解读</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>Problem / Setup</strong>: 训练/评估策略需要环境；真实环境昂贵且不可控，世界模型可作为替代环境。</li>
                            <li><strong>Method</strong>: action-conditioned 的生成式环境 + 以 VLM/判别器等方式构造 reward/评估信号。</li>
                            <li><strong>Evaluation</strong>: 关键是“在世界模型里学到的策略”与“真实世界表现”的相关性。</li>
                            <li><strong>Limitations</strong>: 世界模型偏差会被策略利用（simulator hacking），需要约束与校准。</li>
                        </ul>
                    </div>
                    <div class="commentary">
                        <div class="commentary-title">🧠 ntiGravity's Commentary</div>
                        <p>WorldGym 这类工作提供了一个重要的现实指标：世界模型好不好，不仅看生成质量，更看它是否能支撑<strong>下游决策</strong>且与真实结果一致。对“神经游戏引擎”来说，这是走向可用系统的关键路标。</p>
                    </div>
                </div>
            </div>
        </section>

        <section>
            <h2>📅 2025年8月 (里程碑月)</h2>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2508.13009" target="_blank">Matrix-Game
                                2.0: An Open-Source Real-Time Interactive World Model</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-08-18</span>
                            <span>📄 arXiv:2508.13009</span>
                            <span class="paper-tag github"><a href="https://github.com/SkyworkAI/Matrix-Game"
                                    target="_blank">GitHub</a></span>
                        </div>
                    </div>
                </div>

                <div class="paper-sections">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>可以把它理解成“神经游戏引擎的开源雏形”：你按键盘（动作），模型就生成下一帧画面，速度快到像在玩游戏。关键点是：它让更多团队能复现实验，而不是只看巨头 demo。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解读</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>Problem / Setup</strong>: 要把视频生成变成“环境”，必须 action-conditioned 且能实时推理。</li>
                            <li><strong>Method</strong>: 以 DiT 为骨架，通过因果掩码保证时序生成可用，并用 cross-attention 将动作注入生成过程。</li>
                            <li><strong>Data</strong>: 高质量 action label 与合成数据管线（UE5/GTA5）通常比架构更关键。</li>
                            <li><strong>Evaluation</strong>: 交互可玩性（fps/latency）、动作响应一致性、长时漂移与失败恢复。</li>
                        </ul>
                    </div>
                    <div class="commentary">
                        <div class="commentary-title">🧠 ntiGravity's Commentary</div>
                        <p>这是 2025 年最重要的开源里程碑之一。它证明了“只要数据足够对齐（动作标注 + 可控环境），扩散/Transformer 真的能学到类似引擎的动态”。但它也暴露了硬伤：如果没有显式 memory/state，长时一致性很难靠堆模型解决。</p>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a
                                href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/"
                                target="_blank">Genie 3: A General Purpose World Model (DeepMind Blog)</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-08-05</span>
                            <span>📄 DeepMind Official</span>
                        </div>
                    </div>
                </div>

                <div class="paper-sections">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>Genie 3 把“生成视频”推进到“生成可探索世界”：你可以在里面走动、改变视角，且能在一段时间内保持基本一致性。这就是从“看短片”走向“玩关卡”的关键一步。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解读</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>Problem / Setup</strong>: 需要把环境动态与控制信号建模成可生成的序列，同时保持分钟级一致性与实时感。</li>
                            <li><strong>Method（公开信息）</strong>: 结合高效视频 tokenizer（如 Magvit 系列）与可学习的 latent action，使模型在无显式人工动作标注下获得可控性。</li>
                            <li><strong>Evaluation</strong>: 规格（分辨率/fps/一致性时长）与交互可控性（响应延迟、动作一致性、可回访一致性）。</li>
                            <li><strong>Limitations</strong>: 仍可能主要依赖像素相关性而非显式因果模型；长交互下的 state/memory 读写能力仍是关键瓶颈。</li>
                        </ul>
                    </div>
                    <div class="commentary">
                        <div class="commentary-title">🧠 ntiGravity's Commentary</div>
                        <p>Genie 3 的“天花板”更多是系统工程的胜利：把 tokenizer、动作抽象、推理速度与一致性绑成一套产品级链路。下一步真正拉开差距的，会是显式 state/memory 与可恢复机制（存档/回滚/重锚定）。</p>
                    </div>
                </div>
            </div>
        </section>

        <nav class="chapter-nav">
            <a href="06_companies.html" class="chapter-nav-link prev">
                <span class="chapter-nav-label">← 上一章</span>
                <span class="chapter-nav-title">06. 公司调研 (Companies)</span>
            </a>
            <a href="08_community.html" class="chapter-nav-link next">
                <span class="chapter-nav-label">下一章 →</span>
                <span class="chapter-nav-title">08. 社区动态 (Community)</span>
            </a>
        </nav>
    </main>
</body>

</html>