<!DOCTYPE html>
<html lang="zh-CN">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>07. 论文追踪 - World Model Guide</title>
    <link rel="stylesheet" href="css/style.css">
    <style>
        .paper-card {
            background: white;
            border: 1px solid var(--border);
            border-radius: 0.75rem;
            padding: 1.5rem;
            margin: 1.5rem 0;
            box-shadow: var(--card-shadow);
        }

        .paper-header {
            display: flex;
            justify-content: space-between;
            align-items: flex-start;
            gap: 1rem;
            flex-wrap: wrap;
        }

        .paper-title {
            font-size: 1.2rem;
            font-weight: 700;
            color: var(--text-primary);
            margin-bottom: 0.5rem;
        }

        .paper-title a {
            color: var(--accent);
            text-decoration: none;
        }

        .paper-title a:hover {
            text-decoration: underline;
        }

        .paper-meta {
            font-size: 0.85rem;
            color: var(--text-secondary);
            display: flex;
            gap: 1rem;
            flex-wrap: wrap;
            margin-bottom: 1rem;
        }

        .paper-tag {
            background: #e0f2fe;
            color: #0369a1;
            padding: 0.2rem 0.6rem;
            border-radius: 1rem;
            font-size: 0.75rem;
            font-weight: 600;
        }

        .paper-tag.github {
            background: #f0fdf4;
            color: #15803d;
        }

        .paper-tag.open {
            background: #f0fdf4;
            color: #15803d;
        }

        .paper-tag.closed {
            background: #fee2e2;
            color: #b91c1c;
        }

        .paper-tag.unknown {
            background: #f1f5f9;
            color: #475569;
        }

        .paper-tag.new {
            background: #fef3c7;
            color: #b45309;
        }

        .paper-tag a {
            color: inherit;
            text-decoration: none;
        }

        .paper-tag a:hover {
            text-decoration: underline;
        }

        .dual-view {
            display: flex;
            flex-direction: column;
            gap: 1rem;
            margin: 1rem 0;
        }

        .layman-view {
            background: #f0fae6;
            padding: 1rem;
            border-radius: 0.5rem;
            border-left: 4px solid #22c55e;
        }

        .pro-view {
            background: #fff1f2;
            padding: 1rem;
            border-radius: 0.5rem;
            border-left: 4px solid #e11d48;
        }

        .view-title {
            font-weight: 700;
            font-size: 0.9rem;
            margin-bottom: 0.5rem;
        }

        .layman-view .view-title {
            color: #166534;
        }

        .pro-view .view-title {
            color: #9f1239;
        }

        .last-updated {
            background: #fef3c7;
            border: 1px solid #fcd34d;
            padding: 1rem;
            border-radius: 0.5rem;
            font-size: 0.9rem;
            margin-bottom: 2rem;
        }

        /* 双视角卡片要求：上下排布（🟢在上，🔴在下） */
    </style>
</head>

<body>
    <div class="aurora-bg">
        <div class="aurora-blob aurora-blob-1"></div>
        <div class="aurora-blob aurora-blob-2"></div>
        <div class="aurora-blob aurora-blob-3"></div>
        <div class="aurora-blob aurora-blob-4"></div>
    </div>
    <nav class="sidebar">
        <a href="index.html" class="brand">🚀 World Model Guide</a>
        <ul class="nav-links">
            <li class="nav-item"><a href="index.html" class="nav-link">00. 概览 (Overview)</a></li>
            <li class="nav-item"><a href="01_industry.html" class="nav-link">01. 行业全景 (Landscape)</a></li>
            <li class="nav-item"><a href="02_product.html" class="nav-link">02. 产品深度 (Deep Dive)</a></li>
            <li class="nav-item"><a href="03_architecture.html" class="nav-link">03. 技术架构 (Architecture)</a></li>
            <li class="nav-item"><a href="04_data.html" class="nav-link">04. 数据工程 (Data Bible)</a></li>
            <li class="nav-item"><a href="05_roadmap.html" class="nav-link">05. 落地路线 (Roadmap)</a></li>
            <li class="nav-item"><a href="06_companies.html" class="nav-link">06. 公司调研 (Companies)</a></li>
            <li class="nav-item"><a href="07_paper_tracker.html" class="nav-link active">07. 论文追踪 (Papers)</a></li>
            <li class="nav-item"><a href="08_community.html" class="nav-link">08. 社区动态 (Community)</a></li>
            <li class="nav-item"><a href="references.html" class="nav-link">附录：参考文献 (Refs)</a></li>
        </ul>
    </nav>

    <main class="main-content">
        <h1>07. 最新论文追踪 (Paper Tracker)</h1>
        <p>本页追踪 arXiv/GitHub 上关于<strong>世界模型
                (非具身智能)</strong>、<strong>交互视频生成</strong>、<strong>游戏模拟</strong>的最新研究。按发布日期从新到旧排列。</p>

        <div class="last-updated">
            ⏰ <strong>最后更新时间</strong>: 2026-01-24 | 本页内容每日更新，追踪学术前沿。
        </div>

        <section>
            <h2>📅 2026年1月</h2>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2601.15281" target="_blank">StableWorld: Towards Stable and Consistent Long Interactive Video Generation</a></div>
                        <div class="paper-meta">
                            <span>📅 2026-01-24</span>
                            <span>📄 <a href="https://arxiv.org/abs/2601.15281" target="_blank">arXiv:2601.15281</a></span>
                            <span class="paper-tag"><a href="https://sd-world.github.io/" target="_blank">Project</a></span>
                            <span class="paper-tag github"><a href="https://github.com/xbyym/StableWorld" target="_blank">GitHub</a></span>
                            <span class="paper-tag open"><a href="https://github.com/xbyym/StableWorld" target="_blank">代码开源</a></span>
                            <span class="paper-tag unknown">参数未公开</span>
                            <span class="paper-tag unknown">数据集未公开</span>
                            <span class="paper-tag new">NEW</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>StableWorld 的直觉非常“工程”：世界模型越跑越崩，往往不是因为动作太复杂，而是<strong>同一场景里那一点点“跑偏”在不断累积</strong>。它做的事很像“视频世界的垃圾回收”——滑动窗口里别把已经变脏的历史帧当参考，否则你等于把错误一代代传下去。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>问题根因（不是泛泛而谈）</strong>：作者先在“静态交互”设定下量化漂移：相邻帧差异很小，但在同一场景里逐步累积，最终导致 scene collapse。并用 inter-frame MSE（latent/pixel）展示“越往后越偏离初始干净状态”。<sup><a href="https://arxiv.org/abs/2601.15281" target="_blank">[来源]</a></sup></li>
                            <li><strong>关键观察：窗口变大为啥会更稳？</strong>：扩大 KV-cache window 可以让模型看到更早、更“干净”的帧，从而抑制 drift；但直接加大窗口会显著增算力/降速度。作者进一步指出稳定性主要来自“窗口里保留了几帧干净的 early frames”。<sup><a href="https://arxiv.org/abs/2601.15281" target="_blank">[来源]</a></sup></li>
                            <li><strong>核心贡献：Dynamic Frame Eviction（动态逐出机制）</strong>：不再“窗口一滑就丢最老帧”，而是：<strong>永远保留最近帧</strong>保证局部运动连续；对更早的中间帧，用几何一致性判定哪些是“视角相近但已被污染/冗余”的帧，并优先逐出，避免把 drift 继续写进历史缓冲。<sup><a href="https://arxiv.org/abs/2601.15281" target="_blank">[来源]</a></sup></li>
                            <li><strong>几何一致性打分（可复现公式链）</strong>：以窗口内最早帧 \(P_0\) 为 reference，对中间帧 \(P_k\) 提 ORB 特征，ratio test 得到匹配对 \(G\)，再用 RANSAC 同时拟合 Homography 与 Fundamental matrix，取两者 inlier ratio 的最大值 \(s(P_0,P_k)=\\max(r_H,r_F)\)。若 \(s\\ge\\theta\) 则继续检查更远帧；首次低于阈值时，逐出“失败点前一帧”，否则逐出最远中间帧。论文默认阈值 \(\u03b8=0.75\)。<sup><a href="https://arxiv.org/abs/2601.15281" target="_blank">[来源]</a></sup></li>
                            <li><strong>落地参数（不同模型如何接）</strong>：Matrix-Game KV-cache=9，最早 6 帧为 earlier frames；因每步生成 3 帧，检查第 3/6 帧决定逐出。Open-Oasis window=16，最早 12 帧为 earlier，检查第 1/6/12 帧。Hunyuan-GameCraft 每步生成 33 帧并形成新窗口，按对应帧相似度决定是否把 earlier frames 合并进新窗口。<sup><a href="https://arxiv.org/abs/2601.15281" target="_blank">[来源]</a></sup></li>
                            <li><strong>量化提升（关键数字）</strong>：在 VBench-Long 上，Matrix-Game 的 Image Quality 从 63.39→73.52、Aesthetic 从 38.83→53.44；Open-Oasis 的 Image Quality 66.37→73.75；Hunyuan-GameCraft 的 Aesthetic 48.38→57.44；总体额外开销仅 1.00–1.02× latency。<sup><a href="https://arxiv.org/abs/2601.15281" target="_blank">[来源]</a></sup></li>
                        </ul>

                        <figure style="margin: 0.75rem 0 0;">
                            <figcaption style="font-size: 0.85rem; color: var(--text-secondary); margin-bottom: 0.5rem;">
                                <strong>图：StableWorld 的“动态逐出”滑窗策略（对比 vanilla 丢最老帧）</strong>
                            </figcaption>
                            <svg viewBox="0 0 900 260" width="100%" height="auto" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="StableWorld dynamic frame eviction">
                                <defs>
                                    <marker id="arrowSW" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                        <path d="M0,0 L10,3 L0,6 Z" fill="#64748b" />
                                    </marker>
                                </defs>

                                <text x="20" y="26" font-size="13" fill="#334155">Sliding Window (history)</text>

                                <!-- window blocks -->
                                <rect x="20" y="40" width="120" height="55" rx="10" fill="#f0fae6" stroke="#22c55e" />
                                <text x="32" y="70" font-size="12" fill="#166534" font-weight="700">P0</text>
                                <text x="55" y="70" font-size="12" fill="#166534">Reference</text>

                                <rect x="155" y="40" width="330" height="55" rx="10" fill="#fff1f2" stroke="#e11d48" />
                                <text x="170" y="70" font-size="12" fill="#9f1239" font-weight="700">P1..Pk</text>
                                <text x="230" y="70" font-size="12" fill="#9f1239">Middle frames（可能漂移/冗余）</text>

                                <rect x="500" y="40" width="250" height="55" rx="10" fill="#e0f2fe" stroke="#0369a1" />
                                <text x="515" y="70" font-size="12" fill="#0369a1" font-weight="700">Recent frames</text>
                                <text x="605" y="70" font-size="12" fill="#0369a1">保证局部运动连续</text>

                                <rect x="765" y="40" width="115" height="55" rx="10" fill="#ffffff" stroke="#94a3b8" />
                                <text x="780" y="70" font-size="12" fill="#334155" font-weight="700">New</text>
                                <text x="815" y="70" font-size="12" fill="#334155">Pk+1</text>

                                <line x1="880" y1="67" x2="880" y2="67" stroke="#64748b" stroke-width="2" />

                                <!-- eviction decision -->
                                <rect x="20" y="120" width="860" height="115" rx="12" fill="#f8fafc" stroke="#e2e8f0" />
                                <text x="35" y="148" font-size="13" fill="#0f172a" font-weight="700">Dynamic Frame Eviction</text>
                                <text x="35" y="170" font-size="12" fill="#334155">对 P0 与 Pk 做 ORB 匹配 → RANSAC(H/F) → similarity s=max(rH,rF)。</text>
                                <text x="35" y="190" font-size="12" fill="#334155">若 s≥θ：视角相近（可能冗余且更易积累 drift）→ 优先逐出中间帧；若 s&lt;θ：检测到视角变化/场景切换 → 允许保留新信息。</text>
                                <text x="35" y="212" font-size="12" fill="#334155">对比 vanilla：固定丢最老帧 → “干净参考”更快消失 → drift 更快累积。</text>

                                <line x1="820" y1="95" x2="820" y2="120" stroke="#64748b" stroke-width="2" marker-end="url(#arrowSW)" />
                            </svg>
                        </figure>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2601.12719" target="_blank">S2DiT: Sandwich Diffusion Transformer for Mobile Streaming Video Generation</a></div>
                        <div class="paper-meta">
                            <span>📅 2026-01-24</span>
                            <span>📄 <a href="https://arxiv.org/abs/2601.12719" target="_blank">arXiv:2601.12719</a></span>
                            <span class="paper-tag closed">代码未开源</span>
                            <span class="paper-tag unknown">参数未公开</span>
                            <span class="paper-tag unknown">数据集未公开</span>
                            <span class="paper-tag new">NEW</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>S2DiT 的目标很明确：把“服务器上才能跑的 DiT 视频生成”做成<strong>手机端可流式生成</strong>。它的做法像“三明治”：高分辨率阶段用“便宜但够强的注意力”保细节，低分辨率阶段用“稀疏/下采样注意力”吸收全局信息，再用蒸馏把大模型能力压进一个小模型里。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>目标指标（给出硬数字）</strong>：作者声称在 iPhone 上可<strong>流式 &gt;10 FPS</strong>；表格里 S2DiT-AR 的 streaming 约 <strong>~11 FPS</strong>，VBench 83.26（与服务器级模型接近）。<sup><a href="https://arxiv.org/abs/2601.12719" target="_blank">[来源]</a></sup></li>
                            <li><strong>优化点 1：LCHA（LinConv Hybrid Attention）——线性复杂度但不丢局部</strong>：两条并行分支：
                                - <strong>Linear path</strong>：用可学习的正核 \(\u03d5(x)=softplus(Wx+b)\) 做线性注意力，并配 QK norm 与 3D RoPE；
                                - <strong>Local conv path</strong>：深度可分离 3D conv + 线性混合，且做 temporal causal padding 以支持 streaming；
                                两路输出用门控系数 \(\u03b1\)（FusionGate）融合。<sup><a href="https://arxiv.org/abs/2601.12719" target="_blank">[来源]</a></sup></li>
                            <li><strong>优化点 2：SSA（Stride Self-Attention）——用结构化稀疏换吞吐</strong>：对 QKV 做均匀 stride 下采样，在更低 token 数下建模全局，再通过上采样恢复分辨率；作者强调“只用低分辨率 SSA 会掉质”，因此要与 LCHA 混合。<sup><a href="https://arxiv.org/abs/2601.12719" target="_blank">[来源]</a></sup></li>
                            <li><strong>优化点 3：Budget-aware DP Search（把算力预算变成结构）</strong>：把 block 类型（LCHA/SSA）的 latency \(\u2113_t\) 与 memory \(m_t\) 显式建模，在总 block 数 K 下用动态规划选出 \((N_{LCHA},N_{SSA})\) 使其最贴近设备 \((L_{max},M_{max})\) 预算；再用组级 mask 搜索模块分布（什么时候切换分支）。<sup><a href="https://arxiv.org/abs/2601.12719" target="_blank">[来源]</a></sup></li>
                            <li><strong>优化点 4：2-in-1 Distillation（把大老师“搬到离线”）</strong>：第一阶段做 cached offline distillation：用 Wan 2.2-14B 预计算并缓存 diffusion tuples 与 text embeddings，避免训练时跑 teacher；第二阶段面向 streaming，引入 self-forcing + distribution-matching distillation（少步数）并探索 adversarial fine-tuning 以增强跨 segment 的时序一致性。<sup><a href="https://arxiv.org/abs/2601.12719" target="_blank">[来源]</a></sup></li>
                        </ul>

                        <figure style="margin: 0.75rem 0 0;">
                            <figcaption style="font-size: 0.85rem; color: var(--text-secondary); margin-bottom: 0.5rem;">
                                <strong>图：S2DiT 的“三明治架构 + 两阶段蒸馏”全链路（简化）</strong>
                            </figcaption>
                            <svg viewBox="0 0 900 300" width="100%" height="auto" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="S2DiT sandwich architecture and distillation">
                                <defs>
                                    <marker id="arrowS2" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                        <path d="M0,0 L10,3 L0,6 Z" fill="#64748b" />
                                    </marker>
                                </defs>

                                <rect x="20" y="35" width="340" height="70" rx="12" fill="#fff1f2" stroke="#e11d48" />
                                <text x="35" y="62" font-size="13" fill="#9f1239" font-weight="700">High-Res Stage: LCHA</text>
                                <text x="35" y="82" font-size="12" fill="#9f1239">Linear attention (softplus(Wx+b)) + causal DW-Conv</text>
                                <text x="35" y="100" font-size="12" fill="#9f1239">FusionGate α 融合（保细节，线性复杂度）</text>

                                <rect x="395" y="35" width="240" height="70" rx="12" fill="#e0f2fe" stroke="#0369a1" />
                                <text x="410" y="62" font-size="13" fill="#0369a1" font-weight="700">Low-Res Stage: SSA</text>
                                <text x="410" y="82" font-size="12" fill="#0369a1">Stride downsample QKV</text>
                                <text x="410" y="100" font-size="12" fill="#0369a1">全局上下文更便宜</text>

                                <rect x="670" y="35" width="210" height="70" rx="12" fill="#f0fae6" stroke="#22c55e" />
                                <text x="685" y="62" font-size="13" fill="#166534" font-weight="700">DP Search</text>
                                <text x="685" y="82" font-size="12" fill="#166534">按 Lmax/Mmax 分配</text>
                                <text x="685" y="100" font-size="12" fill="#166534">LCHA/SSA 位置</text>

                                <line x1="360" y1="70" x2="395" y2="70" stroke="#64748b" stroke-width="2" marker-end="url(#arrowS2)" />
                                <line x1="635" y1="70" x2="670" y2="70" stroke="#64748b" stroke-width="2" marker-end="url(#arrowS2)" />

                                <rect x="20" y="135" width="860" height="145" rx="12" fill="#f8fafc" stroke="#e2e8f0" />
                                <text x="35" y="163" font-size="13" fill="#0f172a" font-weight="700">2-in-1 Distillation</text>
                                <text x="35" y="186" font-size="12" fill="#334155">Stage-1 Cached Offline KD：Wan2.2-14B 预计算 diffusion tuples + text embeddings → 监督 student（省 teacher FLOPs/显存）</text>
                                <text x="35" y="208" font-size="12" fill="#334155">Stage-2 Streaming：self-forcing + distribution-matching distillation（少步数）+ adversarial fine-tune（跨 segment 一致）</text>
                                <text x="35" y="232" font-size="12" fill="#334155">结果：few-step AR diffusion，移动端 streaming ~11 FPS（论文表格）</text>
                            </svg>
                        </figure>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2509.22622" target="_blank">LongLive: Real-time Interactive Long Video Generation</a></div>
                        <div class="paper-meta">
                            <span>📅 2026-01-23</span>
                            <span>📄 <a href="https://arxiv.org/abs/2509.22622" target="_blank">arXiv:2509.22622</a></span>
                            <span class="paper-tag github"><a href="https://github.com/NVlabs/LongLive" target="_blank">GitHub</a></span>
                            <span class="paper-tag open"><a href="https://github.com/NVlabs/LongLive" target="_blank">代码开源</a></span>
                            <span class="paper-tag open"><a href="https://huggingface.co/Efficient-Large-Model/LongLive-1.3B" target="_blank">参数开源</a></span>
                            <span class="paper-tag unknown">数据集未公开</span>
                            <span class="paper-tag new">NEW</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>关键词是<strong>“长视频 + 可交互 + 实时”</strong>。它想解决的是：你不只是生成一段 10 秒短片，而是像玩游戏一样，在一个持续很久的“可播放视频世界”里边操作边生成，而且几乎不卡顿。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>核心设定（为什么选 AR）</strong>：LongLive 用 <strong>frame-level causal AR</strong>（因果注意力）替代双向注意力的扩散模型，核心收益是<strong>可以 KV cache</strong>，把“长视频推理成本”从“整段反复算”变成“增量续写”。<sup><a href="https://arxiv.org/abs/2509.22622" target="_blank">[来源]</a></sup></li>
                            <li><strong>贡献 1：KV-recache（解决交互式换 prompt）</strong>：在 prompt 切换边界，如果直接清空 KV cache 会导致画面突变；如果保留全部 cache 又会“粘住旧 prompt”。LongLive 的做法是在边界处<strong>用“已生成帧 + 新 prompt”重新计算/刷新 cache</strong>，把新 prompt 的语义重新写入 cross-attn，再让 self-attn 传播，从而做到“不断片 + 快速贴合新 prompt”。<sup><a href="https://arxiv.org/abs/2509.22622" target="_blank">[来源]</a></sup></li>
                            <li><strong>贡献 2：Streaming Long Tuning（train-long → test-long）</strong>：点名批评传统 AR 视频模型“train-short-test-long”导致越生成越崩；LongLive 明确提出<strong>训练阶段就按流式长视频的方式对齐推理</strong>，缓解 train-test gap。<sup><a href="https://arxiv.org/abs/2509.22622" target="_blank">[来源]</a></sup></li>
                            <li><strong>贡献 3：Short-window Attention + Frame Sink（加速且保一致）</strong>：把注意力窗口缩短以提高吞吐，并引入<strong>frame-level attention sink（frame sink）</strong>保留长程一致性（让远处信息“有地方被看见/被汇总”）。<sup><a href="https://arxiv.org/abs/2509.22622" target="_blank">[来源]</a></sup></li>
                            <li><strong>工程指标（必须看）</strong>：论文报告 1.3B 模型 minute-long 微调仅 32 GPU-days；推理在单张 H100 上<strong>20.7 FPS</strong>，最长支持<strong>240 秒</strong>，并支持 INT8 量化推理（质量损失很小）。<sup><a href="https://arxiv.org/abs/2509.22622" target="_blank">[来源]</a></sup></li>
                        </ul>

                        <figure style="margin: 0.75rem 0 0;">
                            <figcaption style="font-size: 0.85rem; color: var(--text-secondary); margin-bottom: 0.5rem;">
                                <strong>图：LongLive 的“交互式换 Prompt + KV-recache”流程（简化示意）</strong>
                            </figcaption>
                            <svg viewBox="0 0 900 260" width="100%" height="auto" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="LongLive KV-recache pipeline">
                                <defs>
                                    <marker id="arrow" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                        <path d="M0,0 L10,3 L0,6 Z" fill="#64748b" />
                                    </marker>
                                </defs>
                                <rect x="20" y="30" width="210" height="70" rx="10" fill="#f0fae6" stroke="#22c55e" />
                                <text x="35" y="58" font-size="14" fill="#166534" font-weight="700">Prompt Stream</text>
                                <text x="35" y="78" font-size="12" fill="#166534">P1 → P2 → P3（运行中切换）</text>

                                <rect x="260" y="30" width="260" height="70" rx="10" fill="#fff1f2" stroke="#e11d48" />
                                <text x="275" y="58" font-size="14" fill="#9f1239" font-weight="700">Frame-level Causal AR (DiT-like)</text>
                                <text x="275" y="78" font-size="12" fill="#9f1239">Self-Attn（因果） + Cross-Attn（prompt）</text>

                                <rect x="550" y="30" width="150" height="70" rx="10" fill="#e0f2fe" stroke="#0369a1" />
                                <text x="565" y="58" font-size="14" fill="#0369a1" font-weight="700">KV Cache</text>
                                <text x="565" y="78" font-size="12" fill="#0369a1">增量续写加速</text>

                                <rect x="730" y="30" width="150" height="70" rx="10" fill="#ffffff" stroke="#94a3b8" />
                                <text x="745" y="58" font-size="14" fill="#334155" font-weight="700">Video Output</text>
                                <text x="745" y="78" font-size="12" fill="#334155">实时帧流</text>

                                <line x1="230" y1="65" x2="260" y2="65" stroke="#64748b" stroke-width="2" marker-end="url(#arrow)" />
                                <line x1="520" y1="65" x2="550" y2="65" stroke="#64748b" stroke-width="2" marker-end="url(#arrow)" />
                                <line x1="700" y1="65" x2="730" y2="65" stroke="#64748b" stroke-width="2" marker-end="url(#arrow)" />

                                <rect x="260" y="140" width="620" height="90" rx="12" fill="#f8fafc" stroke="#e2e8f0" />
                                <text x="275" y="170" font-size="14" fill="#0f172a" font-weight="700">Prompt Switch Boundary</text>
                                <text x="275" y="192" font-size="12" fill="#334155">KV-recache：用「已生成帧」+「新 prompt」重新写入/刷新 cross-attn 相关 KV</text>
                                <text x="275" y="212" font-size="12" fill="#334155">目标：不断片（continuity）+ 快速贴合新 prompt（adherence）</text>

                                <line x1="520" y1="100" x2="520" y2="140" stroke="#64748b" stroke-width="2" marker-end="url(#arrow)" />
                            </svg>
                        </figure>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://github.com/Wan-Video/Wan2.2" target="_blank">WAN 2.2（开源视频基础模型套件）</a></div>
                        <div class="paper-meta">
                            <span>📅 2026-01-23</span>
                            <span>📄 Tech Report: <a href="https://arxiv.org/abs/2503.20314" target="_blank">arXiv:2503.20314</a></span>
                            <span class="paper-tag github"><a href="https://github.com/Wan-Video/Wan2.2" target="_blank">GitHub</a></span>
                            <span class="paper-tag open"><a href="https://github.com/Wan-Video/Wan2.2" target="_blank">代码开源</a></span>
                            <span class="paper-tag open"><a href="https://huggingface.co/Wan-AI" target="_blank">参数开源</a></span>
                            <span class="paper-tag unknown">数据集未公开</span>
                            <span class="paper-tag new">NEW</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>可以把 WAN 2.2 当作对 2.1 的“工程化升级”：更像产品可用的版本，重点通常会落在<strong>更稳的长时一致性</strong>与<strong>更低的生成延迟</strong>，以及更完整的控制能力。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>改进点 1：MoE（Mixture-of-Experts）扩容但不涨算力</strong>：官方 README 明确强调把 <strong>MoE 引入视频扩散</strong>，并用“跨 timestep 的专门 expert”拆分去噪过程，使总体容量变大但计算成本维持不变。<sup><a href="https://github.com/Wan-Video/Wan2.2" target="_blank">[来源]</a></sup></li>
                            <li><strong>改进点 2：美学可控（数据 + 标签）</strong>：引入精细美学数据与标签（光照、构图、对比度、色调等），使“电影感风格”更可控而不仅是随机抽样到好看。<sup><a href="https://github.com/Wan-Video/Wan2.2" target="_blank">[来源]</a></sup></li>
                            <li><strong>改进点 3：复杂运动泛化（数据扩容）</strong>：相比 Wan2.1，训练数据规模提升（+65.6% images，+83.2% videos），旨在提升 motion / semantics / aesthetics 的泛化。<sup><a href="https://github.com/Wan-Video/Wan2.2" target="_blank">[来源]</a></sup></li>
                            <li><strong>关键工程信号：720P@24fps 的 TI2V-5B + 高压缩 VAE</strong>：Wan2.2 开源一个 5B 的 hybrid TI2V 模型，配套 Wan2.2-VAE 的压缩比 <strong>16×16×4</strong>，支持 720P/24fps，并宣称可在 4090 等消费卡运行。<sup><a href="https://github.com/Wan-Video/Wan2.2" target="_blank">[来源]</a></sup></li>
                        </ul>

                        <figure style="margin: 0.75rem 0 0;">
                            <figcaption style="font-size: 0.85rem; color: var(--text-secondary); margin-bottom: 0.5rem;">
                                <strong>图：WAN 2.2 “跨 timestep 的 MoE 去噪”示意（简化）</strong>
                            </figcaption>
                            <svg viewBox="0 0 900 210" width="100%" height="auto" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="Wan2.2 MoE denoising across timesteps">
                                <defs>
                                    <marker id="arrow2" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                        <path d="M0,0 L10,3 L0,6 Z" fill="#64748b" />
                                    </marker>
                                </defs>
                                <text x="20" y="26" font-size="13" fill="#334155">Noise level / timestep: high → medium → low</text>
                                <line x1="20" y1="45" x2="880" y2="45" stroke="#94a3b8" stroke-width="2" marker-end="url(#arrow2)"/>

                                <rect x="40" y="70" width="240" height="55" rx="10" fill="#fff1f2" stroke="#e11d48"/>
                                <text x="55" y="102" font-size="13" fill="#9f1239" font-weight="700">Expert A (high-noise)</text>
                                <text x="55" y="120" font-size="11" fill="#9f1239">粗结构 / 大轮廓</text>

                                <rect x="330" y="70" width="240" height="55" rx="10" fill="#fff1f2" stroke="#e11d48"/>
                                <text x="345" y="102" font-size="13" fill="#9f1239" font-weight="700">Expert B (mid-noise)</text>
                                <text x="345" y="120" font-size="11" fill="#9f1239">运动/语义细化</text>

                                <rect x="620" y="70" width="240" height="55" rx="10" fill="#fff1f2" stroke="#e11d48"/>
                                <text x="635" y="102" font-size="13" fill="#9f1239" font-weight="700">Expert C (low-noise)</text>
                                <text x="635" y="120" font-size="11" fill="#9f1239">纹理 / 电影感细节</text>

                                <line x1="280" y1="98" x2="330" y2="98" stroke="#64748b" stroke-width="2" marker-end="url(#arrow2)"/>
                                <line x1="570" y1="98" x2="620" y2="98" stroke="#64748b" stroke-width="2" marker-end="url(#arrow2)"/>

                                <rect x="40" y="145" width="820" height="45" rx="10" fill="#f8fafc" stroke="#e2e8f0"/>
                                <text x="55" y="173" font-size="12" fill="#334155">MoE 的产品含义：容量↑（质量/泛化潜力）但每步算力≈不变；真正瓶颈转向 VAE/注意力/采样步数等系统侧优化。</text>
                            </svg>
                        </figure>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2503.20314" target="_blank">WAN 2.1（技术报告：Wan 视频生成基础模型）</a></div>
                        <div class="paper-meta">
                            <span>📅 2026-01-23</span>
                            <span>📄 <a href="https://arxiv.org/abs/2503.20314" target="_blank">arXiv:2503.20314</a></span>
                            <span class="paper-tag github"><a href="https://github.com/Wan-Video/Wan2.1" target="_blank">GitHub</a></span>
                            <span class="paper-tag open"><a href="https://github.com/Wan-Video/Wan2.1" target="_blank">代码开源</a></span>
                            <span class="paper-tag open"><a href="https://huggingface.co/Wan-AI" target="_blank">参数开源</a></span>
                            <span class="paper-tag unknown">数据集未公开</span>
                            <span class="paper-tag new">NEW</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>WAN 2.1 更像“打底版本”：先把画面质量与基础可控性做上去，但在长视频和强交互场景里，通常还会遇到“玩久了就跑偏”的问题。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>系统拆解：VAE（决定 token 长度/吞吐的第一层）</strong>：Wan 报告提出 <strong>Wan-VAE（3D causal VAE）</strong>，把视频从 \([1+T, H, W, 3]\) 压缩到 \([1+T/4, H/8, W/8, C]\)，即<strong>时域 4×、空域 8×8×</strong> 的压缩；并强调第一帧只做空间压缩以更好兼容图像先验。<sup><a href="https://arxiv.org/abs/2503.20314" target="_blank">[来源]</a></sup></li>
                            <li><strong>关键优化 1：因果性 + RMSNorm（让“流式编码/解码”成立）</strong>：用 RMSNorm 替换 GroupNorm 以保持时序因果，从而可实现<strong>feature cache</strong>机制，提高长视频编码/解码效率。<sup><a href="https://arxiv.org/abs/2503.20314" target="_blank">[来源]</a></sup></li>
                            <li><strong>关键优化 2：Chunk-wise + Feature Cache（支持任意长视频 VAE 推理）</strong>：将输入按 \(1+T/4\) 的 chunk 处理，每个 chunk 最多 4 帧；用“上一 chunk 的末尾特征”作为缓存拼到下一 chunk，避免长序列显存爆炸。论文还给出 kernel=3 时缓存两帧的示意。<sup><a href="https://arxiv.org/abs/2503.20314" target="_blank">[来源]</a></sup></li>
                            <li><strong>训练细节（不是泛泛而谈）</strong>：Wan-VAE 采用三阶段：先训 2D image VAE → inflate 成 3D causal VAE → 高质量视频微调；损失包含 L1、KL、LPIPS（权重分别 3、3e-6、3），最终加入 3D discriminator 的 GAN loss。<sup><a href="https://arxiv.org/abs/2503.20314" target="_blank">[来源]</a></sup></li>
                            <li><strong>2.1 ↔ 2.2 的对比抓手（从“瓶颈”角度）</strong>：如果 2.2 的 MoE 扩容提升质量/泛化，那么要想真正跨到“可交互门槛”，还得同时在<strong>采样步数、注意力/缓存、VAE 解码</strong>上做系统级协同（TurboDiffusion 就是典型例子）。<sup><a href="https://arxiv.org/abs/2512.16093" target="_blank">[来源]</a></sup></li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2601.05138" target="_blank">VerseCrafter: Dynamic Realistic Video World Model with 4D Geometric Control</a></div>
                        <div class="paper-meta">
                            <span>📅 2026-01-08</span>
                            <span>📄 <a href="https://arxiv.org/abs/2601.05138" target="_blank">arXiv:2601.05138</a></span>
                            <span class="paper-tag"><a href="https://sixiaozheng.github.io/VerseCrafter_page/" target="_blank">Project</a></span>
                            <span class="paper-tag github"><a href="https://github.com/TencentARC/VerseCrafter" target="_blank">GitHub</a></span>
                            <span class="paper-tag open"><a href="https://github.com/TencentARC/VerseCrafter" target="_blank">代码开源</a></span>
                            <span class="paper-tag open"><a href="https://huggingface.co/TencentARC/VerseCrafter" target="_blank">参数开源</a></span>
                            <span class="paper-tag unknown">数据集未公开</span>
                            <span class="paper-tag new">NEW</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>VerseCrafter 把“生成视频”升级成“生成一个带 4D 世界状态的视频”。你不再只用文字说“往左转、车开过去”，而是直接给模型一个<strong>可编辑的 4D 几何控制信号</strong>：相机怎么走、每个物体怎么动，统一在同一个世界坐标里。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>核心贡献：4D Geometric Control（统一相机 + 多物体运动）</strong>：用<strong>静态背景点云</strong>表达场景几何，用<strong>每个物体的 3D Gaussian 轨迹</strong>表达动态（均值=路径，协方差=体积/朝向/占据概率），相比 3D box/SMPL 等更<strong>柔性、类目无关、可编辑</strong>。<sup><a href="https://arxiv.org/abs/2601.05138" target="_blank">[来源]</a></sup></li>
                            <li><strong>模型注入方式：GeoAdapter（ControlNet 风格轻量分支）</strong>：将 4D 控制渲染成多视角 conditioning signal，并通过 GeoAdapter 注入到<strong>冻结的 Wan2.1-14B 视频扩散骨干</strong>，以“几何驱动”方式约束相机与物体运动，同时保留大模型的视觉先验。<sup><a href="https://arxiv.org/abs/2601.05138" target="_blank">[来源]</a></sup></li>
                            <li><strong>数据瓶颈的解决：VerseControl4D 自动标注引擎</strong>：从 in-the-wild 视频自动提取相机/物体轨迹并构建 4D 控制，从而获得大规模训练数据（解决“真实世界没有 4D 标注”的硬伤）。<sup><a href="https://arxiv.org/abs/2601.05138" target="_blank">[来源]</a></sup></li>
                            <li><strong>评测关注点（别只看画质）</strong>：是否真正<strong>按指定相机轨迹与多物体轨迹走</strong>（view-consistency、遮挡鲁棒性），并与 Yume/Uni3C 等 3D-aware baselines 对比。<sup><a href="https://sixiaozheng.github.io/VerseCrafter_page/" target="_blank">[来源]</a></sup></li>
                        </ul>

                        <figure style="margin: 0.75rem 0 0;">
                            <figcaption style="font-size: 0.85rem; color: var(--text-secondary); margin-bottom: 0.5rem;">
                                <strong>图：VerseCrafter 的“4D 控制 → 渲染 → 注入 → 生成”链路（简化示意）</strong>
                            </figcaption>
                            <svg viewBox="0 0 900 260" width="100%" height="auto" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="VerseCrafter pipeline">
                                <defs>
                                    <marker id="arrow3" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                        <path d="M0,0 L10,3 L0,6 Z" fill="#64748b" />
                                    </marker>
                                </defs>
                                <rect x="20" y="35" width="260" height="70" rx="10" fill="#f0fae6" stroke="#22c55e" />
                                <text x="35" y="62" font-size="14" fill="#166534" font-weight="700">4D Geometric Control</text>
                                <text x="35" y="82" font-size="12" fill="#166534">背景点云 + 物体 3D Gaussian 轨迹</text>

                                <rect x="310" y="35" width="190" height="70" rx="10" fill="#e0f2fe" stroke="#0369a1" />
                                <text x="325" y="62" font-size="14" fill="#0369a1" font-weight="700">Render to Views</text>
                                <text x="325" y="82" font-size="12" fill="#0369a1">conditioning maps</text>

                                <rect x="530" y="35" width="150" height="70" rx="10" fill="#fff1f2" stroke="#e11d48" />
                                <text x="545" y="62" font-size="14" fill="#9f1239" font-weight="700">GeoAdapter</text>
                                <text x="545" y="82" font-size="12" fill="#9f1239">(ControlNet-style)</text>

                                <rect x="710" y="35" width="170" height="70" rx="10" fill="#ffffff" stroke="#94a3b8" />
                                <text x="725" y="62" font-size="14" fill="#334155" font-weight="700">Frozen Wan2.1-14B</text>
                                <text x="725" y="82" font-size="12" fill="#334155">Video Diffusion Backbone</text>

                                <line x1="280" y1="70" x2="310" y2="70" stroke="#64748b" stroke-width="2" marker-end="url(#arrow3)" />
                                <line x1="500" y1="70" x2="530" y2="70" stroke="#64748b" stroke-width="2" marker-end="url(#arrow3)" />
                                <line x1="680" y1="70" x2="710" y2="70" stroke="#64748b" stroke-width="2" marker-end="url(#arrow3)" />

                                <rect x="20" y="145" width="860" height="90" rx="12" fill="#f8fafc" stroke="#e2e8f0" />
                                <text x="35" y="175" font-size="13" fill="#0f172a" font-weight="700">为什么 3D Gaussian 轨迹比 2D/3D Box 更好用？</text>
                                <text x="35" y="197" font-size="12" fill="#334155">（1）在世界坐标里统一相机与物体（view-consistent）；（2）协方差=软占据，更适配真实物体形状；（3）可编辑、类目无关。</text>
                            </svg>
                        </figure>
                    </div>
                </div>
            </div>
        </section>

        <section>
            <h2>📅 2025年12月</h2>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2512.18619"
                                target="_blank">ChronoDreamer: Action-Conditioned World Model as an Online Simulator for Robotic Planning</a>
                        </div>
                        <div class="paper-meta">
                            <span>📅 2025-12-23</span>
                            <span>📄 <a href="https://arxiv.org/abs/2512.18619" target="_blank">arXiv:2512.18619</a></span>
                            <span class="paper-tag closed">代码未开源</span>
                            <span class="paper-tag unknown">参数未公开</span>
                            <span class="paper-tag unknown">数据集未公开</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>ChronoDreamer 让机器人可以"做白日梦"来练习操作。它能根据机械臂的动作指令，想象出接下来会发生什么画面，帮助机器人在行动前先"脑补"一遍。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>架构</strong>: Spatio-temporal Transformer，多模态输入 (RGB, Contact Map, Joint State)。
                            </li>
                            <li><strong>特点</strong>: 针对高频接触的机器人操作场景优化。</li>
                            <li><strong>注意</strong>: 属于具身智能边界，但 World Model 架构可复用。</li>
                        </ul>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2512.16093"
                                target="_blank">TurboDiffusion: Accelerating Video Diffusion by 100-200x</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-12-20</span>
                            <span>📄 <a href="https://arxiv.org/abs/2512.16093" target="_blank">arXiv:2512.16093</a></span>
                            <span class="paper-tag github"><a href="https://github.com/thu-ml/TurboDiffusion" target="_blank">GitHub</a></span>
                            <span class="paper-tag open"><a href="https://github.com/thu-ml/TurboDiffusion" target="_blank">代码开源</a></span>
                            <span class="paper-tag open"><a href="https://huggingface.co/TurboDiffusion" target="_blank">参数开源</a></span>
                            <span class="paper-tag unknown">数据集未公开</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>这是一个"加速外挂"。它能把原本要 1 分钟生成的 AI 视频，缩短到不到 1 秒，而且画质几乎不变。是 <a href="https://www.vidu.studio/"
                                target="_blank">Vidu</a> 实时生成的核心技术。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>目标（“100–200×”到底加速了什么）</strong>：加速的是<strong>扩散采样阶段</strong>的端到端 latency（论文说明统计时不含 text encoding 与 VAE decoding）。<sup><a href="https://arxiv.org/abs/2512.16093" target="_blank">[来源]</a></sup></li>
                            <li><strong>加速点 1：Attention 加速（SageAttention2++ + SLA）</strong>：先用可训练的 <strong>Sparse-Linear Attention (SLA)</strong> 替换 full attention 并做适配微调；推理阶段再用基于 SageAttention 的 CUDA 实现（论文称 SageSLA）把“稀疏性”与“低比特 TensorCore”叠加起来。<sup><a href="https://arxiv.org/abs/2512.16093" target="_blank">[来源]</a></sup></li>
                            <li><strong>加速点 2：步数蒸馏（rCM）</strong>：用 rCM 做 step distillation，把采样步数从 100 缩到 4/3（论文实践里用 3 steps，并建议在 4 steps 下更稳）。<sup><a href="https://arxiv.org/abs/2512.16093" target="_blank">[来源]</a></sup></li>
                            <li><strong>加速点 3：W8A8（线性层 INT8）</strong>：权重与激活都量化到 INT8，且采用 block-wise 粒度（论文给出 block size <strong>128×128</strong>），配合 Tensor Cores 提升线性层吞吐并将模型体积约减半。<sup><a href="https://arxiv.org/abs/2512.16093" target="_blank">[来源]</a></sup></li>
                            <li><strong>关键超参（决定“快”与“崩”）</strong>：Top-K ratio=0.1（对应 90% attention sparsity）、3 sampling steps；作者建议 Top-K \([0.1, 0.15]\) 且 steps=4 更稳定。<sup><a href="https://arxiv.org/abs/2512.16093" target="_blank">[来源]</a></sup></li>
                            <li><strong>量化结果（不是一句话）</strong>：在单张 RTX 5090 上，对 Wan2.1/2.2 多个模型报告接近 100–200× 的速度提升（例如 Wan2.1-T2V-14B-720P 的 final 版本速度提升到 199×），并给出逐项叠加（CPU offload → W8A8 & fused norm → rCM → SageSLA）。<sup><a href="https://arxiv.org/abs/2512.16093" target="_blank">[来源]</a></sup></li>
                        </ul>

                        <figure style="margin: 0.75rem 0 0;">
                            <figcaption style="font-size: 0.85rem; color: var(--text-secondary); margin-bottom: 0.5rem;">
                                <strong>图：TurboDiffusion 的“训练侧两条线 → 权重合并 → 推理侧加速栈”</strong>
                            </figcaption>
                            <svg viewBox="0 0 900 300" width="100%" height="auto" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="TurboDiffusion pipeline">
                                <defs>
                                    <marker id="arrowTD" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                        <path d="M0,0 L10,3 L0,6 Z" fill="#64748b" />
                                    </marker>
                                </defs>

                                <rect x="20" y="30" width="260" height="60" rx="10" fill="#ffffff" stroke="#94a3b8" />
                                <text x="35" y="58" font-size="14" fill="#334155" font-weight="700">Pretrained Video Diffusion Model</text>
                                <text x="35" y="77" font-size="12" fill="#334155">（Wan2.1/2.2 等）</text>

                                <line x1="280" y1="60" x2="330" y2="60" stroke="#64748b" stroke-width="2" marker-end="url(#arrowTD)" />

                                <rect x="330" y="15" width="250" height="80" rx="12" fill="#fff1f2" stroke="#e11d48" />
                                <text x="345" y="43" font-size="13" fill="#9f1239" font-weight="700">Branch A (Attention)</text>
                                <text x="345" y="63" font-size="12" fill="#9f1239">FullAttn → SLA（训练适配）</text>
                                <text x="345" y="81" font-size="12" fill="#9f1239">推理用 SageSLA（CUDA + 低比特）</text>

                                <rect x="330" y="120" width="250" height="80" rx="12" fill="#fff1f2" stroke="#e11d48" />
                                <text x="345" y="148" font-size="13" fill="#9f1239" font-weight="700">Branch B (Steps)</text>
                                <text x="345" y="168" font-size="12" fill="#9f1239">rCM 做 step distillation</text>
                                <text x="345" y="186" font-size="12" fill="#9f1239">100 steps → 4/3 steps</text>

                                <line x1="580" y1="60" x2="640" y2="100" stroke="#64748b" stroke-width="2" marker-end="url(#arrowTD)" />
                                <line x1="580" y1="160" x2="640" y2="120" stroke="#64748b" stroke-width="2" marker-end="url(#arrowTD)" />

                                <rect x="640" y="85" width="240" height="70" rx="12" fill="#e0f2fe" stroke="#0369a1" />
                                <text x="655" y="112" font-size="13" fill="#0369a1" font-weight="700">Merge Weights</text>
                                <text x="655" y="132" font-size="12" fill="#0369a1">SLA finetune + rCM updates</text>

                                <rect x="20" y="225" width="860" height="60" rx="12" fill="#f8fafc" stroke="#e2e8f0" />
                                <text x="35" y="253" font-size="13" fill="#0f172a" font-weight="700">Inference Stack（最终加速栈）</text>
                                <text x="35" y="273" font-size="12" fill="#334155">SageSLA（attention） + rCM（少步数） + W8A8（线性层 INT8, 128×128） + fused norm/其它 kernel 优化</text>

                                <line x1="760" y1="155" x2="760" y2="225" stroke="#64748b" stroke-width="2" marker-end="url(#arrowTD)" />
                            </svg>
                        </figure>
                    </div>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2512.04040"
                                target="_blank">RELIC: Interactive Video World Model with Long-Horizon Memory</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-12-15 (Preprint)</span>
                            <span>📄 <a href="https://arxiv.org/abs/2512.04040" target="_blank">arXiv:2512.04040</a></span>
                            <span class="paper-tag closed">代码未开源</span>
                            <span class="paper-tag unknown">参数未公开</span>
                            <span class="paper-tag unknown">数据集未公开</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>RELIC 让 AI 生成的视频世界拥有"长期记忆"。你走出房间再回来，东西还会在原位。这是目前视频模型最缺失的能力之一。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>关键问题</strong>: 解决 Long-horizon 场景下的 Temporal Consistency。</li>
                            <li><strong>方法</strong>: 引入显式 Memory Bank，而不仅依赖 Autoregressive 上下文。</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <section>
            <h2>📅 2025年9月</h2>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://openreview.net/forum?id=worldgym"
                                target="_blank">WorldGym: World Model as an Environment for Policy Evaluation</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-09-19</span>
                            <span>📄 OpenReview (NeurIPS Submission)</span>
                            <span class="paper-tag closed">代码未开源</span>
                            <span class="paper-tag unknown">参数未公开</span>
                            <span class="paper-tag unknown">数据集未公开</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>WorldGym 把世界模型变成了一个"虚拟健身房"。AI Agent 可以在里面反复练习，而不用真的去操作机器人或开真车，省钱又安全。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>架构</strong>: Autoregressive, Action-conditioned 视频生成。</li>
                            <li><strong>评估方式</strong>: 使用 VLM (Vision-Language Model) 作为 Reward Function。</li>
                            <li><strong>验证</strong>: 在 World Model 中的策略成功率与真实世界高度相关。</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <section>
            <h2>📅 2025年8月 (里程碑月)</h2>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a href="https://arxiv.org/abs/2508.13009" target="_blank">Matrix-Game
                                2.0: An Open-Source Real-Time Interactive World Model</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-08-18</span>
                            <span>📄 <a href="https://arxiv.org/abs/2508.13009" target="_blank">arXiv:2508.13009</a></span>
                            <span class="paper-tag github"><a href="https://github.com/SkyworkAI/Matrix-Game" target="_blank">GitHub</a></span>
                            <span class="paper-tag open"><a href="https://github.com/SkyworkAI/Matrix-Game" target="_blank">代码开源</a></span>
                            <span class="paper-tag open"><a href="https://huggingface.co/Skywork/Matrix-Game-2.0" target="_blank">参数开源</a></span>
                            <span class="paper-tag unknown">数据集未公开</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>这就是把 GTA5 喂给了一个画图 AI。当你按 "W" 键时，它就画出下一帧向前的画面。因为画得非常快 (25
                            FPS)，所以你感觉像在玩游戏。<strong>而且是开源的！</strong></p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>问题定义（为什么“实时交互”难）</strong>：双向注意力 + 多步去噪导致每帧都要看“全片”，延迟爆炸；而纯 AR 又容易误差累积导致越玩越崩。<sup><a href="https://arxiv.org/abs/2508.13009" target="_blank">[来源]</a></sup></li>
                            <li><strong>贡献 1：数据管线（这篇最硬的护城河）</strong>：提出 UE + GTA5 的可规模化数据生产与标注，论文给出规模约 <strong>~1200 hours</strong> 的带交互标注视频数据；UE 管线包含 NavMesh 路径规划（增强轨迹多样性）、相机控制的 Quaternion 精度优化等；GTA5 侧通过 Script Hook 级集成做“画面-操作”同步录制。<sup><a href="https://arxiv.org/abs/2508.13009" target="_blank">[来源]</a></sup></li>
                            <li><strong>贡献 2：模型骨干 + 动作注入</strong>：Video Diffusion Transformer 内集成 action control module，把键鼠输入做 frame-level 条件，通过注入模块让“动作→画面变化”具有因果一致性。<sup><a href="https://arxiv.org/abs/2508.13009" target="_blank">[来源]</a></sup></li>
                            <li><strong>贡献 3：Few-step auto-regressive diffusion（KV cache + 少步数）</strong>：使用 causal few-step AR diffusion，并基于 Self-Forcing 训练范式把“推理时 KV cache 的滚动生成”对齐到训练中，从而在速度与稳定性之间取平衡。<sup><a href="https://arxiv.org/abs/2508.13009" target="_blank">[来源]</a></sup></li>
                            <li><strong>硬指标（别只说“很快”）</strong>：论文报告在单张 H100 上可达 <strong>25 FPS</strong>，并支持 minute-level 视频滚动生成。<sup><a href="https://arxiv.org/abs/2508.13009" target="_blank">[来源]</a></sup></li>
                        </ul>

                        <figure style="margin: 0.75rem 0 0;">
                            <figcaption style="font-size: 0.85rem; color: var(--text-secondary); margin-bottom: 0.5rem;">
                                <strong>图：Matrix-Game 2.0 从“数据生产 → 训练 → 实时交互推理”全链路（简化示意）</strong>
                            </figcaption>
                            <svg viewBox="0 0 900 290" width="100%" height="auto" xmlns="http://www.w3.org/2000/svg" role="img" aria-label="Matrix-Game pipeline">
                                <defs>
                                    <marker id="arrowMG" markerWidth="10" markerHeight="10" refX="9" refY="3" orient="auto">
                                        <path d="M0,0 L10,3 L0,6 Z" fill="#64748b" />
                                    </marker>
                                </defs>

                                <rect x="20" y="30" width="250" height="70" rx="12" fill="#f0fae6" stroke="#22c55e" />
                                <text x="35" y="58" font-size="13" fill="#166534" font-weight="700">Data Engine</text>
                                <text x="35" y="78" font-size="12" fill="#166534">UE：NavMesh/相机控制/自动采集</text>
                                <text x="35" y="96" font-size="12" fill="#166534">GTA5：Script Hook 同步录制</text>

                                <rect x="305" y="30" width="275" height="70" rx="12" fill="#e0f2fe" stroke="#0369a1" />
                                <text x="320" y="58" font-size="13" fill="#0369a1" font-weight="700">Dataset (~1200h)</text>
                                <text x="320" y="78" font-size="12" fill="#0369a1">frame-level video + action labels</text>
                                <text x="320" y="96" font-size="12" fill="#0369a1">keyboard/mouse + camera signals</text>

                                <rect x="615" y="30" width="265" height="70" rx="12" fill="#fff1f2" stroke="#e11d48" />
                                <text x="630" y="58" font-size="13" fill="#9f1239" font-weight="700">Training</text>
                                <text x="630" y="78" font-size="12" fill="#9f1239">DiT backbone + action injection</text>
                                <text x="630" y="96" font-size="12" fill="#9f1239">Self-Forcing distillation</text>

                                <line x1="270" y1="65" x2="305" y2="65" stroke="#64748b" stroke-width="2" marker-end="url(#arrowMG)" />
                                <line x1="580" y1="65" x2="615" y2="65" stroke="#64748b" stroke-width="2" marker-end="url(#arrowMG)" />

                                <rect x="20" y="145" width="860" height="120" rx="12" fill="#f8fafc" stroke="#e2e8f0" />
                                <text x="35" y="175" font-size="13" fill="#0f172a" font-weight="700">Interactive Inference Loop（实时交互）</text>
                                <text x="35" y="197" font-size="12" fill="#334155">Input: 初始帧 + 动作流（WASD/鼠标） → Causal AR Diffusion（少步数 + KV cache） → 25 FPS 输出</text>
                                <text x="35" y="220" font-size="12" fill="#334155">核心难点：误差累积（drift）与长时稳定性；Self-Forcing 用“训练期模拟推理滚动”缓解 train-test gap。</text>
                            </svg>
                        </figure>
                    </div>
                </div>

                <div class="commentary">
                    <div class="commentary-title">🧠 AntiGravity's Commentary</div>
                    <p>这是 2025 年最重要的开源项目之一。它证明了只要你有足够的 UE/GTA 合成数据，Diffusion Model 真的能学会物理引擎的逻辑。<strong>缺点</strong>:
                        没有显式 Memory Bank，长时一致性差。</p>
                </div>
            </div>

            <div class="paper-card">
                <div class="paper-header">
                    <div>
                        <div class="paper-title"><a
                                href="https://deepmind.google/discover/blog/genie-3-a-new-frontier-for-world-models/"
                                target="_blank">Genie 3: A General Purpose World Model (DeepMind Blog)</a></div>
                        <div class="paper-meta">
                            <span>📅 2025-08-05</span>
                            <span>📄 DeepMind Official</span>
                            <span class="paper-tag closed">代码未开源</span>
                            <span class="paper-tag unknown">参数未公开</span>
                            <span class="paper-tag unknown">数据集未公开</span>
                        </div>
                    </div>
                </div>

                <div class="dual-view">
                    <div class="layman-view">
                        <div class="view-title">🟢 通俗解读</div>
                        <p>Genie 3 能把你的一句话变成一个可探索的 3D 梦境。你可以像玩游戏一样在里面走动，而且走出房间再回来，东西还在原位。这是目前的天花板。</p>
                    </div>
                    <div class="pro-view">
                        <div class="view-title">🔴 专业解析</div>
                        <ul style="padding-left: 1rem; margin: 0;">
                            <li><strong>规格</strong>: 720p / 24 FPS / 分钟级一致性。</li>
                            <li><strong>Tokenizer</strong>: Magvit-v2 (LFQ)。</li>
                            <li><strong>控制</strong>: Latent Action (无需人工标注)。</li>
                            <li><strong>意义</strong>: 从"动图"到"可玩关卡"的分水岭。</li>
                        </ul>
                    </div>
                </div>
            </div>
        </section>

        <nav class="chapter-nav">
            <a href="06_companies.html" class="chapter-nav-link prev">
                <span class="chapter-nav-label">← 上一章</span>
                <span class="chapter-nav-title">06. 公司调研 (Companies)</span>
            </a>
            <a href="08_community.html" class="chapter-nav-link next">
                <span class="chapter-nav-label">下一章 →</span>
                <span class="chapter-nav-title">08. 社区动态 (Community)</span>
            </a>
        </nav>
    </main>
</body>

</html>